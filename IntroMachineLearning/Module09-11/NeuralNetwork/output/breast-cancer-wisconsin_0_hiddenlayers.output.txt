options: {'missing_attributes': 'drop', 'left_index': True, 'normalize_cols': {2: 'clump_thickness', 3: 'cell_size_uniformity', 4: 'cell_size_shape', 5: 'marginal_adhesion', 6: 'single_epithesial_cell_size', 7: 'bare_nuclei', 8: 'bland_chromatin', 9: 'normal_nucleoti', 10: 'mitoses'}, 'result_col': 11, 'order_results': True}
     clump_thickness  cell_size_uniformity  cell_size_shape  marginal_adhesion  single_epithesial_cell_size  bare_nuclei  bland_chromatin  normal_nucleoti   mitoses  result
0          -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000       2
427        -1.000000             -1.000000        -1.000000          -0.555556                    -0.777778    -0.555556        -1.000000        -1.000000 -1.000000       2
428        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -0.777778        -1.000000        -1.000000 -1.000000       2
429        -0.111111             -1.000000        -1.000000           0.111111                    -0.555556    -1.000000        -0.777778        -1.000000 -1.000000       2
430        -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
431        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
432        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
426        -0.111111             -0.777778        -0.777778          -0.333333                    -0.777778    -0.333333        -1.000000        -1.000000 -1.000000       2
433        -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -1.000000        -1.000000 -1.000000       2
436        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
437        -0.555556             -1.000000        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
439        -0.777778             -0.555556        -1.000000          -1.000000                    -0.555556    -1.000000        -1.000000        -1.000000 -1.000000       2
443        -0.111111             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
444        -0.111111             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
445        -0.111111             -1.000000        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
435        -0.333333             -1.000000        -1.000000          -0.555556                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000       2
446        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -0.111111        -1.000000        -1.000000 -1.000000       2
424        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
422        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
403        -0.111111             -0.777778        -0.777778          -0.777778                    -0.777778    -0.777778        -0.555556        -0.777778 -0.777778       2
404        -0.777778             -0.555556        -1.000000          -1.000000                    -0.111111    -1.000000        -1.000000        -1.000000 -1.000000       2
405        -0.555556             -0.777778        -0.777778          -0.555556                    -0.777778    -0.555556        -0.555556        -1.000000 -1.000000       2
407        -0.333333             -0.555556        -0.555556          -1.000000                    -0.777778    -1.000000        -0.555556        -0.555556 -1.000000       2
408        -0.111111             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000       2
409        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
423        -0.333333             -1.000000        -0.555556          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
411        -0.111111             -0.555556         0.111111          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000       2
414        -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000       2
415        -1.000000             -0.555556        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -0.777778 -1.000000       2
416        -0.111111             -1.000000        -1.000000          -0.555556                    -0.333333    -1.000000        -0.555556        -0.777778 -1.000000       2
..               ...                   ...              ...                ...                          ...          ...              ...              ...       ...     ...
377         1.000000              0.555556         0.333333          -0.333333                    -0.555556     1.000000         0.333333         0.777778 -1.000000       4
155         1.000000              0.333333         0.333333          -0.333333                    -0.111111     1.000000        -0.111111         0.333333 -0.777778       4
154         0.777778              0.777778         1.000000          -0.555556                     0.111111     1.000000         0.333333         1.000000  0.111111       4
290         1.000000              1.000000         1.000000           0.333333                     0.777778     1.000000         0.333333         1.000000  1.000000       4
151        -0.111111             -0.111111        -0.111111           0.111111                    -0.555556     1.000000        -0.555556        -1.000000 -1.000000       4
491         0.555556              1.000000         1.000000           1.000000                     0.333333    -0.111111        -0.333333         0.555556  0.333333       4
148         1.000000              1.000000         0.555556           0.111111                    -0.333333    -0.111111         0.555556         1.000000 -1.000000       4
147         0.333333             -0.777778        -0.333333          -1.000000                     0.111111     1.000000        -0.111111        -0.333333 -0.555556       4
239         0.555556              1.000000         1.000000           0.555556                    -0.111111     1.000000         0.333333         0.555556 -1.000000       4
145         0.555556              0.555556         0.333333          -0.333333                     1.000000     1.000000         0.333333         0.555556  0.333333       4
372         0.333333              0.111111         0.111111          -0.555556                    -0.777778     1.000000         0.333333        -1.000000 -1.000000       4
240         0.555556             -0.333333        -0.333333          -1.000000                    -0.777778     0.777778        -0.555556        -0.555556 -1.000000       4
142        -0.555556             -0.333333        -0.111111          -0.777778                     0.111111     0.555556        -0.333333        -1.000000 -1.000000       4
124         0.333333             -0.111111        -0.555556           0.333333                    -0.333333     1.000000         0.333333        -0.111111 -0.111111       4
499         0.111111              0.111111         0.333333           1.000000                    -0.555556     1.000000         0.555556         1.000000 -0.777778       4
139         0.777778             -0.111111        -0.111111          -0.333333                    -0.333333    -0.111111        -0.333333        -0.555556 -0.555556       4
504        -0.333333              0.333333         0.555556          -0.555556                    -0.333333     1.000000         0.777778        -1.000000 -1.000000       4
243         1.000000             -0.333333        -0.333333           1.000000                    -0.777778     1.000000        -0.111111        -0.555556 -0.555556       4
367         1.000000              0.111111        -0.555556           0.111111                    -0.333333     1.000000         0.333333         0.555556 -0.333333       4
507         1.000000             -0.333333        -0.111111          -0.333333                    -0.555556    -0.111111         0.333333        -0.555556 -1.000000       4
508         0.333333             -0.111111         0.111111           1.000000                    -0.333333     1.000000        -0.111111        -0.555556 -1.000000       4
245         0.111111              1.000000         1.000000          -0.777778                     0.555556     1.000000         0.333333        -0.555556 -0.555556       4
246         0.777778              1.000000         1.000000          -1.000000                     1.000000     0.555556        -0.555556        -0.555556 -1.000000       4
247        -0.111111              0.111111         0.111111          -0.777778                    -0.333333     1.000000        -0.555556         0.111111 -1.000000       4
515         0.333333             -0.333333        -0.333333          -0.555556                    -0.333333     1.000000         0.111111         0.777778 -1.000000       4
130        -0.111111              1.000000         0.555556           1.000000                     0.555556     1.000000        -0.555556         0.111111 -0.555556       4
252         1.000000             -0.111111         0.555556           1.000000                    -0.555556     1.000000        -0.111111        -1.000000 -0.555556       4
126         0.555556             -0.555556        -0.111111          -0.333333                    -0.111111     1.000000        -1.000000         0.111111 -0.777778       4
500        -0.333333              1.000000        -0.333333           0.333333                    -0.555556     1.000000         0.777778         1.000000 -1.000000       4
682        -0.333333              0.555556         0.555556          -0.111111                    -0.333333    -0.111111         1.000000        -0.333333 -1.000000       4

[683 rows x 10 columns]
This is a classification set
class key: {0: 2.0, 1: 4.0}
the dataset is: 
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, 0.11111111111111116, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.7777777777777778, -0.7777777777777778, -0.33333333333333337, -0.7777777777777778, -0.33333333333333337, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -0.5555555555555556, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -0.11111111111111116, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, 0]
[-0.7777777777777778, -0.5555555555555556, -1.0, -1.0, -0.11111111111111116, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -0.5555555555555556, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -0.5555555555555556, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.5555555555555556, -1.0, 0]
[-0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, 0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.5555555555555556, -0.33333333333333337, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[0.11111111111111116, 0.7777777777777777, 0.33333333333333326, -0.11111111111111116, -0.11111111111111116, 0.5555555555555556, -0.33333333333333337, -0.7777777777777778, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, 1.0, -0.33333333333333337, -0.11111111111111116, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -0.33333333333333337, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -0.5555555555555556, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, -0.7777777777777778, -0.33333333333333337, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -0.5555555555555556, -0.7777777777777778, 0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.11111111111111116, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, -0.5555555555555556, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -1.0, -0.33333333333333337, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -0.33333333333333337, -0.33333333333333337, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -0.33333333333333337, -0.11111111111111116, -0.5555555555555556, 0.33333333333333326, -0.5555555555555556, -0.33333333333333337, 0.11111111111111116, -1.0, 0]
[-1.0, -1.0, -0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -0.11111111111111116, -0.11111111111111116, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.5555555555555556, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, 0]
[-0.11111111111111116, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.33333333333333337, -0.33333333333333337, -0.33333333333333337, -0.33333333333333337, 0.11111111111111116, -0.11111111111111116, 0.33333333333333326, -0.5555555555555556, -1.0, 0]
[-0.33333333333333337, -0.5555555555555556, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-1.0, -0.7777777777777778, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -0.7777777777777778, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -0.33333333333333337, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.33333333333333337, -0.5555555555555556, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, -0.33333333333333337, -1.0, -0.33333333333333337, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -0.5555555555555556, 0]
[-0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -0.5555555555555556, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -0.7777777777777778, -0.33333333333333337, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -0.5555555555555556, -0.33333333333333337, -1.0, -1.0, -1.0, 0]
[-1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.7777777777777778, 0.11111111111111116, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[0.33333333333333326, -1.0, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -0.5555555555555556, -1.0, -0.5555555555555556, -0.33333333333333337, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -0.11111111111111116, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[0.11111111111111116, -0.7777777777777778, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.5555555555555556, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, 0.5555555555555556, 0]
[-1.0, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -0.33333333333333337, -0.11111111111111116, -1.0, 0.5555555555555556, -1.0, -0.5555555555555556, 0.11111111111111116, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -0.33333333333333337, -0.7777777777777778, -1.0, 0]
[-0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -0.33333333333333337, -0.7777777777777778, -1.0, -0.7777777777777778, -0.11111111111111116, -0.7777777777777778, -1.0, -0.7777777777777778, 0]
[-1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -1.0, -0.33333333333333337, 0.5555555555555556, -1.0, 0]
[-0.11111111111111116, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, 0.11111111111111116, -0.5555555555555556, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -0.7777777777777778, -0.33333333333333337, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.5555555555555556, 0.5555555555555556, -1.0, -0.11111111111111116, 0.5555555555555556, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -0.11111111111111116, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -0.5555555555555556, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.33333333333333337, -0.5555555555555556, -1.0, -1.0, -1.0, 0]
[0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -1.0, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0.33333333333333326, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.33333333333333337, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -0.11111111111111116, -0.33333333333333337, 0.33333333333333326, 0.33333333333333326, 0.5555555555555556, -0.7777777777777778, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -0.11111111111111116, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-1.0, -0.7777777777777778, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -1.0, -0.7777777777777778, -0.5555555555555556, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.5555555555555556, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -0.5555555555555556, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -0.11111111111111116, 0]
[-0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, 1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[0.11111111111111116, 0.5555555555555556, 0.5555555555555556, -1.0, -0.5555555555555556, -0.33333333333333337, -0.5555555555555556, 0.33333333333333326, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -0.33333333333333337, -0.33333333333333337, -0.11111111111111116, 0.33333333333333326, 1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[0.11111111111111116, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0.33333333333333326, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, 0.33333333333333326, -1.0, -1.0, 0]
[-1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.11111111111111116, -0.5555555555555556, -0.7777777777777778, 0]
[-0.11111111111111116, -0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, -1.0, 0.33333333333333326, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -0.5555555555555556, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -0.7777777777777778, -0.33333333333333337, -0.7777777777777778, -1.0, 0]
[-1.0, -0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -1.0, 0.33333333333333326, -0.7777777777777778, -1.0, 0]
[-0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.33333333333333337, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, 0.33333333333333326, 0.33333333333333326, -1.0, -0.11111111111111116, 0.5555555555555556, -0.5555555555555556, -0.33333333333333337, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[0.5555555555555556, -0.33333333333333337, 0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -1.0, -0.33333333333333337, -0.5555555555555556, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.11111111111111116, -0.5555555555555556, 1.0, -0.5555555555555556, -0.11111111111111116, -0.5555555555555556, 0]
[-0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -0.5555555555555556, 0.11111111111111116, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-0.33333333333333337, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, 0.11111111111111116, -1.0, 0]
[-0.7777777777777778, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.5555555555555556, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -0.11111111111111116, -0.11111111111111116, -1.0, -1.0, 0]
[-0.5555555555555556, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -1.0, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-0.11111111111111116, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[0.5555555555555556, -0.7777777777777778, -1.0, -1.0, -0.11111111111111116, -1.0, -1.0, -1.0, -1.0, 0]
[-0.11111111111111116, -0.5555555555555556, -0.33333333333333337, -0.5555555555555556, -0.33333333333333337, -0.11111111111111116, -0.33333333333333337, 0.33333333333333326, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.7777777777777778, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[-1.0, -1.0, -1.0, -1.0, -0.7777777777777778, -1.0, -0.5555555555555556, -1.0, -1.0, 0]
[0.7777777777777777, -0.33333333333333337, -0.11111111111111116, 1.0, 0.11111111111111116, 1.0, -0.33333333333333337, 0.5555555555555556, -1.0, 1]
[1.0, -0.5555555555555556, -0.5555555555555556, -1.0, -0.7777777777777778, 1.0, 0.33333333333333326, 0.11111111111111116, -1.0, 1]
[0.11111111111111116, 1.0, -0.7777777777777778, 0.5555555555555556, 1.0, -0.7777777777777778, 0.33333333333333326, 0.5555555555555556, 1.0, 1]
[0.5555555555555556, -0.11111111111111116, -0.11111111111111116, -0.11111111111111116, -0.7777777777777778, 1.0, -0.33333333333333337, -0.5555555555555556, -1.0, 1]
[-0.33333333333333337, 0.5555555555555556, 0.11111111111111116, -0.33333333333333337, -0.5555555555555556, -0.33333333333333337, 1.0, 0.11111111111111116, -1.0, 1]
[0.5555555555555556, -0.5555555555555556, 0.5555555555555556, -0.5555555555555556, -0.33333333333333337, 0.7777777777777777, 0.5555555555555556, 0.7777777777777777, 0.5555555555555556, 1]
[-0.33333333333333337, 1.0, 0.5555555555555556, -0.11111111111111116, -0.33333333333333337, -1.0, 1.0, -1.0, -1.0, 1]
[-0.11111111111111116, -0.5555555555555556, -0.33333333333333337, -1.0, 0.5555555555555556, 1.0, -0.33333333333333337, 0.7777777777777777, -1.0, 1]
[-0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -1.0, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, 1]
[1.0, -0.33333333333333337, -0.7777777777777778, -1.0, -0.5555555555555556, -0.7777777777777778, -0.33333333333333337, -0.5555555555555556, 1.0, 1]
[0.11111111111111116, -0.11111111111111116, -0.11111111111111116, 0.5555555555555556, -0.33333333333333337, 1.0, -0.5555555555555556, -0.33333333333333337, -1.0, 1]
[0.5555555555555556, 1.0, 1.0, 0.33333333333333326, 1.0, 1.0, 0.33333333333333326, -0.5555555555555556, 0.5555555555555556, 1]
[0.5555555555555556, 1.0, -0.11111111111111116, -0.5555555555555556, 0.5555555555555556, -0.33333333333333337, -0.33333333333333337, 1.0, -0.5555555555555556, 1]
[0.11111111111111116, -0.5555555555555556, -0.33333333333333337, -1.0, -0.11111111111111116, -0.7777777777777778, -0.5555555555555556, 0.7777777777777777, -1.0, 1]
[0.7777777777777777, 1.0, 1.0, -1.0, 1.0, 0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -1.0, 1]
[-0.33333333333333337, 0.11111111111111116, 0.11111111111111116, -0.11111111111111116, 0.33333333333333326, 0.11111111111111116, 0.33333333333333326, 0.33333333333333326, -0.5555555555555556, 1]
[-1.0, -0.33333333333333337, -0.5555555555555556, 1.0, -0.33333333333333337, 1.0, -0.11111111111111116, 0.11111111111111116, -1.0, 1]
[-0.11111111111111116, -0.5555555555555556, -0.11111111111111116, -0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.33333333333333337, 1.0, -1.0, 1]
[1.0, 0.7777777777777777, 0.33333333333333326, -0.5555555555555556, -0.33333333333333337, -0.7777777777777778, 0.33333333333333326, 0.33333333333333326, -1.0, 1]
[0.5555555555555556, 1.0, 1.0, 1.0, 0.11111111111111116, 1.0, 1.0, 1.0, 1.0, 1]
[-0.7777777777777778, 0.33333333333333326, 1.0, 1.0, 0.33333333333333326, 1.0, -0.33333333333333337, 0.7777777777777777, -0.33333333333333337, 1]
[1.0, 0.11111111111111116, -0.33333333333333337, -1.0, -0.5555555555555556, -0.33333333333333337, -0.5555555555555556, -0.7777777777777778, -0.5555555555555556, 1]
[-0.11111111111111116, 1.0, 1.0, 1.0, -0.11111111111111116, -0.7777777777777778, 0.5555555555555556, -0.11111111111111116, -1.0, 1]
[-0.5555555555555556, -0.5555555555555556, 0.11111111111111116, -0.33333333333333337, -0.11111111111111116, 0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -1.0, 1]
[1.0, -0.5555555555555556, -0.33333333333333337, -0.11111111111111116, -0.5555555555555556, 1.0, -0.33333333333333337, -1.0, -1.0, 1]
[0.7777777777777777, 0.5555555555555556, 0.5555555555555556, 0.7777777777777777, 0.11111111111111116, -0.5555555555555556, -0.33333333333333337, -1.0, -1.0, 1]
[-0.11111111111111116, 1.0, 0.11111111111111116, -1.0, 1.0, -0.33333333333333337, -0.33333333333333337, 1.0, 1.0, 1]
[-0.33333333333333337, 0.5555555555555556, 0.11111111111111116, -0.5555555555555556, -0.33333333333333337, 1.0, 0.33333333333333326, -1.0, -1.0, 1]
[-0.5555555555555556, -0.11111111111111116, 0.33333333333333326, 0.5555555555555556, 0.5555555555555556, 0.7777777777777777, 0.33333333333333326, 1.0, 0.33333333333333326, 1]
[-0.33333333333333337, -0.7777777777777778, -0.5555555555555556, -0.11111111111111116, -0.5555555555555556, 0.5555555555555556, 0.33333333333333326, 0.11111111111111116, -1.0, 1]
[-0.5555555555555556, -0.33333333333333337, -0.33333333333333337, 1.0, -0.11111111111111116, -1.0, -0.5555555555555556, -0.5555555555555556, -1.0, 1]
[0.33333333333333326, -0.7777777777777778, -0.33333333333333337, -1.0, -0.5555555555555556, -0.33333333333333337, -0.5555555555555556, -0.5555555555555556, -1.0, 1]
[-0.5555555555555556, 0.11111111111111116, 0.11111111111111116, 0.11111111111111116, -0.11111111111111116, 1.0, 0.11111111111111116, 0.5555555555555556, -0.5555555555555556, 1]
[0.5555555555555556, 1.0, 1.0, 1.0, 0.11111111111111116, 1.0, 1.0, 1.0, -1.0, 1]
[-0.5555555555555556, -0.5555555555555556, -0.11111111111111116, -0.7777777777777778, -0.5555555555555556, 1.0, 0.33333333333333326, -1.0, -1.0, 1]
[0.5555555555555556, -0.33333333333333337, 0.33333333333333326, -1.0, -0.5555555555555556, 1.0, -0.5555555555555556, 0.7777777777777777, -0.7777777777777778, 1]
[-0.11111111111111116, -0.33333333333333337, 0.11111111111111116, 0.5555555555555556, -0.33333333333333337, -1.0, 0.5555555555555556, 1.0, -1.0, 1]
[-0.11111111111111116, -0.5555555555555556, -0.7777777777777778, 0.5555555555555556, -0.11111111111111116, 1.0, 0.5555555555555556, -1.0, -0.7777777777777778, 1]
[1.0, -0.11111111111111116, 1.0, -0.5555555555555556, -0.11111111111111116, 0.5555555555555556, 0.33333333333333326, 0.5555555555555556, -0.5555555555555556, 1]
[0.11111111111111116, 1.0, -0.11111111111111116, -0.11111111111111116, -0.33333333333333337, 1.0, 0.11111111111111116, 1.0, -1.0, 1]
[1.0, -0.11111111111111116, 0.33333333333333326, -0.5555555555555556, -0.5555555555555556, 0.33333333333333326, -0.5555555555555556, -0.5555555555555556, 0.5555555555555556, 1]
[-0.11111111111111116, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1]
[-0.7777777777777778, -0.11111111111111116, 0.33333333333333326, 0.11111111111111116, -0.33333333333333337, 1.0, 0.33333333333333326, 0.11111111111111116, -1.0, 1]
[1.0, -0.33333333333333337, -0.5555555555555556, 1.0, -0.5555555555555556, 1.0, 0.33333333333333326, -1.0, -0.7777777777777778, 1]
[-0.11111111111111116, 0.33333333333333326, 1.0, 0.11111111111111116, -0.11111111111111116, 1.0, 0.33333333333333326, -0.11111111111111116, -1.0, 1]
[0.33333333333333326, 0.11111111111111116, -0.33333333333333337, 0.5555555555555556, 1.0, 1.0, 0.7777777777777777, -0.11111111111111116, -0.5555555555555556, 1]
[0.5555555555555556, 0.33333333333333326, 0.5555555555555556, 0.33333333333333326, -0.11111111111111116, -0.11111111111111116, -0.11111111111111116, 1.0, -0.7777777777777778, 1]
[-0.11111111111111116, -0.7777777777777778, -0.5555555555555556, -1.0, 0.11111111111111116, 1.0, -0.11111111111111116, -1.0, -1.0, 1]
[-0.11111111111111116, -0.33333333333333337, 0.11111111111111116, 1.0, -0.7777777777777778, 1.0, -0.33333333333333337, -1.0, -1.0, 1]
[0.11111111111111116, -1.0, -0.5555555555555556, -1.0, -0.33333333333333337, -0.11111111111111116, -0.11111111111111116, 1.0, -1.0, 1]
[-0.11111111111111116, -0.7777777777777778, -0.5555555555555556, -0.33333333333333337, -0.7777777777777778, 0.33333333333333326, -0.5555555555555556, 0.11111111111111116, -1.0, 1]
[0.33333333333333326, 0.11111111111111116, -0.5555555555555556, -0.7777777777777778, -0.11111111111111116, 1.0, 0.33333333333333326, -0.33333333333333337, 0.11111111111111116, 1]
[-0.11111111111111116, 0.33333333333333326, -0.33333333333333337, -1.0, 0.11111111111111116, -1.0, 0.33333333333333326, 1.0, -0.5555555555555556, 1]
[-0.11111111111111116, 1.0, 1.0, 0.5555555555555556, -0.11111111111111116, -0.11111111111111116, 0.33333333333333326, 1.0, -1.0, 1]
[-0.5555555555555556, 1.0, 0.33333333333333326, 0.5555555555555556, -0.11111111111111116, 0.5555555555555556, 0.33333333333333326, -0.33333333333333337, -1.0, 1]
[-0.11111111111111116, 0.11111111111111116, 0.11111111111111116, 0.5555555555555556, 0.11111111111111116, 1.0, -0.33333333333333337, 1.0, -0.33333333333333337, 1]
[1.0, -0.11111111111111116, -0.11111111111111116, -0.5555555555555556, 0.11111111111111116, 0.33333333333333326, 0.33333333333333326, 1.0, -1.0, 1]
[0.33333333333333326, -0.5555555555555556, -0.7777777777777778, 1.0, -0.11111111111111116, 1.0, -0.11111111111111116, -0.33333333333333337, -0.33333333333333337, 1]
[1.0, 0.33333333333333326, 0.33333333333333326, 0.11111111111111116, -0.33333333333333337, 1.0, -0.33333333333333337, -1.0, -0.7777777777777778, 1]
[0.11111111111111116, 0.5555555555555556, 0.33333333333333326, 0.5555555555555556, 0.11111111111111116, 0.5555555555555556, 0.5555555555555556, 0.7777777777777777, -1.0, 1]
[-0.11111111111111116, -0.11111111111111116, -0.11111111111111116, -0.7777777777777778, -0.11111111111111116, 1.0, -0.33333333333333337, -0.5555555555555556, -1.0, 1]
[0.33333333333333326, -0.33333333333333337, 0.11111111111111116, -0.33333333333333337, 0.11111111111111116, -1.0, -0.33333333333333337, -0.5555555555555556, -1.0, 1]
[0.5555555555555556, 0.33333333333333326, -0.11111111111111116, 1.0, 0.33333333333333326, 0.7777777777777777, -0.11111111111111116, -0.11111111111111116, -0.33333333333333337, 1]
[1.0, 1.0, 1.0, 1.0, -0.11111111111111116, 1.0, 1.0, 1.0, 0.33333333333333326, 1]
[-0.11111111111111116, 1.0, 1.0, 1.0, -0.33333333333333337, 1.0, -0.11111111111111116, 0.11111111111111116, -0.5555555555555556, 1]
[-0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.7777777777777778, -0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -1.0, 1]
[1.0, -0.33333333333333337, -0.33333333333333337, 0.11111111111111116, -0.7777777777777778, 1.0, -0.7777777777777778, -0.5555555555555556, -1.0, 1]
[0.5555555555555556, 0.11111111111111116, -0.33333333333333337, 1.0, 1.0, -1.0, -0.5555555555555556, -0.11111111111111116, -1.0, 1]
[-0.11111111111111116, -0.11111111111111116, 0.33333333333333326, 0.5555555555555556, 0.11111111111111116, 1.0, 0.33333333333333326, -0.33333333333333337, -1.0, 1]
[0.33333333333333326, 0.5555555555555556, 0.33333333333333326, 0.11111111111111116, -0.33333333333333337, -0.5555555555555556, 0.5555555555555556, 0.5555555555555556, -0.33333333333333337, 1]
[0.5555555555555556, 1.0, 1.0, 0.5555555555555556, 0.33333333333333326, 1.0, 0.7777777777777777, 0.33333333333333326, -1.0, 1]
[-0.11111111111111116, 1.0, 1.0, -0.11111111111111116, -0.33333333333333337, -0.11111111111111116, -0.33333333333333337, -0.33333333333333337, -1.0, 1]
[0.7777777777777777, -1.0, -0.7777777777777778, 0.11111111111111116, -0.33333333333333337, 1.0, 0.33333333333333326, 0.33333333333333326, -0.7777777777777778, 1]
[0.5555555555555556, -0.33333333333333337, 1.0, -0.11111111111111116, -0.33333333333333337, -0.33333333333333337, 0.33333333333333326, 1.0, -1.0, 1]
[1.0, 0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -0.33333333333333337, 1.0, -0.5555555555555556, 1.0, -0.33333333333333337, 1]
[0.5555555555555556, -0.5555555555555556, -0.33333333333333337, 0.7777777777777777, -0.5555555555555556, 1.0, -0.5555555555555556, -0.5555555555555556, -1.0, 1]
[-0.11111111111111116, 1.0, 1.0, -0.5555555555555556, 0.33333333333333326, -0.5555555555555556, 0.5555555555555556, 1.0, -0.7777777777777778, 1]
[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -0.33333333333333337, 1.0, 1.0, 1]
[0.7777777777777777, -0.11111111111111116, -0.11111111111111116, -0.7777777777777778, -0.7777777777777778, -0.7777777777777778, -0.11111111111111116, -1.0, -1.0, 1]
[1.0, -1.0, -1.0, -1.0, -0.7777777777777778, 1.0, -0.11111111111111116, -0.33333333333333337, -1.0, 1]
[1.0, 0.33333333333333326, 0.33333333333333326, -0.5555555555555556, 0.5555555555555556, -0.11111111111111116, 0.33333333333333326, -0.33333333333333337, -0.5555555555555556, 1]
[0.5555555555555556, -0.7777777777777778, -0.33333333333333337, -1.0, -0.11111111111111116, -1.0, -0.11111111111111116, -0.33333333333333337, -0.33333333333333337, 1]
[0.5555555555555556, 1.0, 1.0, -1.0, -0.5555555555555556, 0.11111111111111116, -0.5555555555555556, 0.7777777777777777, -1.0, 1]
[0.5555555555555556, 0.33333333333333326, -0.33333333333333337, -0.33333333333333337, -0.11111111111111116, -0.5555555555555556, -0.11111111111111116, 1.0, -1.0, 1]
[1.0, -0.5555555555555556, -0.11111111111111116, -0.33333333333333337, -0.5555555555555556, 0.33333333333333326, -0.5555555555555556, -0.11111111111111116, -0.5555555555555556, 1]
[1.0, 0.11111111111111116, 0.11111111111111116, -0.5555555555555556, -0.33333333333333337, -0.11111111111111116, -0.5555555555555556, 0.11111111111111116, -1.0, 1]
[1.0, -0.11111111111111116, -0.11111111111111116, 0.11111111111111116, 0.5555555555555556, 0.5555555555555556, 0.33333333333333326, -1.0, -1.0, 1]
[1.0, 1.0, 0.33333333333333326, 0.5555555555555556, 0.33333333333333326, -1.0, 1.0, 1.0, -0.5555555555555556, 1]
[-0.11111111111111116, -0.11111111111111116, -0.11111111111111116, 0.5555555555555556, 1.0, 0.5555555555555556, 0.33333333333333326, -0.5555555555555556, 0.33333333333333326, 1]
[1.0, -0.5555555555555556, 0.11111111111111116, -0.7777777777777778, -0.5555555555555556, -0.11111111111111116, -0.33333333333333337, 1.0, -0.7777777777777778, 1]
[-0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.33333333333333337, -0.7777777777777778, -0.33333333333333337, -0.5555555555555556, -0.33333333333333337, -1.0, 1]
[0.7777777777777777, -0.11111111111111116, 0.5555555555555556, -1.0, -0.7777777777777778, -0.5555555555555556, -0.7777777777777778, -1.0, -0.11111111111111116, 1]
[0.33333333333333326, 0.5555555555555556, 0.33333333333333326, -0.7777777777777778, -0.33333333333333337, 0.5555555555555556, -0.5555555555555556, 0.5555555555555556, -0.7777777777777778, 1]
[0.5555555555555556, 0.11111111111111116, 0.33333333333333326, -0.5555555555555556, -0.5555555555555556, 1.0, -0.5555555555555556, -0.33333333333333337, -0.7777777777777778, 1]
[-0.11111111111111116, -0.33333333333333337, 0.11111111111111116, 0.11111111111111116, -0.33333333333333337, 1.0, -0.33333333333333337, -0.5555555555555556, -1.0, 1]
[-0.5555555555555556, 0.33333333333333326, 0.33333333333333326, -0.33333333333333337, -0.33333333333333337, 0.7777777777777777, -0.33333333333333337, 0.5555555555555556, -1.0, 1]
[1.0, -0.33333333333333337, 0.11111111111111116, -1.0, -0.7777777777777778, 1.0, -0.11111111111111116, -0.5555555555555556, -1.0, 1]
[1.0, 1.0, 1.0, -0.33333333333333337, 0.5555555555555556, -1.0, 0.5555555555555556, 1.0, -1.0, 1]
[-0.11111111111111116, 0.11111111111111116, -0.11111111111111116, 0.11111111111111116, 1.0, -1.0, -0.5555555555555556, -1.0, -1.0, 1]
[-0.11111111111111116, 1.0, 1.0, 1.0, 1.0, -0.7777777777777778, 1.0, 1.0, 1.0, 1]
[0.11111111111111116, 1.0, 1.0, -0.7777777777777778, 0.5555555555555556, 1.0, 0.33333333333333326, -0.5555555555555556, -0.5555555555555556, 1]
[1.0, -0.33333333333333337, -0.5555555555555556, -1.0, -0.5555555555555556, -0.5555555555555556, 0.11111111111111116, -0.11111111111111116, -0.7777777777777778, 1]
[-0.7777777777777778, -0.11111111111111116, -0.5555555555555556, -0.5555555555555556, 0.11111111111111116, 0.33333333333333326, 0.33333333333333326, -0.11111111111111116, -1.0, 1]
[-0.11111111111111116, -0.33333333333333337, -0.33333333333333337, 0.7777777777777777, -0.7777777777777778, 1.0, -0.11111111111111116, 0.11111111111111116, -1.0, 1]
[0.33333333333333326, -0.33333333333333337, -0.11111111111111116, 1.0, -0.7777777777777778, 1.0, -0.5555555555555556, 0.5555555555555556, -0.7777777777777778, 1]
[1.0, 1.0, 1.0, 0.5555555555555556, 0.11111111111111116, -1.0, 0.5555555555555556, 0.7777777777777777, -1.0, 1]
[1.0, -0.33333333333333337, 0.33333333333333326, -0.7777777777777778, -0.7777777777777778, 0.5555555555555556, 0.11111111111111116, -1.0, -1.0, 1]
[1.0, -0.33333333333333337, 0.11111111111111116, -0.33333333333333337, -0.11111111111111116, 1.0, 0.33333333333333326, -1.0, -1.0, 1]
[0.5555555555555556, 1.0, -0.5555555555555556, -0.7777777777777778, 0.11111111111111116, -0.33333333333333337, -0.5555555555555556, 1.0, -1.0, 1]
[0.33333333333333326, 0.5555555555555556, 0.5555555555555556, 0.33333333333333326, -0.5555555555555556, 1.0, 0.33333333333333326, -0.7777777777777778, -0.5555555555555556, 1]
[0.5555555555555556, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 0.33333333333333326, -0.5555555555555556, 1]
[0.7777777777777777, 0.11111111111111116, 0.7777777777777777, -0.7777777777777778, 1.0, 0.11111111111111116, -0.7777777777777778, 0.7777777777777777, 1.0, 1]
[0.33333333333333326, -0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.7777777777777778, 0.33333333333333326, 1]
[0.5555555555555556, 1.0, -0.33333333333333337, -0.33333333333333337, 0.5555555555555556, 1.0, 0.5555555555555556, -0.7777777777777778, -1.0, 1]
[1.0, -0.11111111111111116, 0.11111111111111116, 1.0, 0.11111111111111116, 1.0, 0.33333333333333326, 0.33333333333333326, 1.0, 1]
[-1.0, -0.11111111111111116, 0.5555555555555556, 0.11111111111111116, -0.11111111111111116, 0.5555555555555556, 0.33333333333333326, 1.0, -1.0, 1]
[1.0, 1.0, 1.0, -0.5555555555555556, 1.0, 1.0, 0.7777777777777777, 1.0, -1.0, 1]
[1.0, 0.7777777777777777, 0.5555555555555556, 0.33333333333333326, 0.11111111111111116, -0.33333333333333337, 0.33333333333333326, 1.0, -0.5555555555555556, 1]
[1.0, 0.11111111111111116, 0.11111111111111116, -0.7777777777777778, -0.33333333333333337, 1.0, 0.7777777777777777, 0.33333333333333326, -1.0, 1]
[0.11111111111111116, 0.11111111111111116, 0.11111111111111116, -0.11111111111111116, -0.33333333333333337, 1.0, 0.33333333333333326, 0.11111111111111116, -0.7777777777777778, 1]
[0.5555555555555556, 0.33333333333333326, 0.11111111111111116, -0.33333333333333337, -0.33333333333333337, 1.0, -0.11111111111111116, -1.0, -1.0, 1]
[-0.11111111111111116, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, -0.11111111111111116, 1.0, 0.33333333333333326, 0.5555555555555556, -1.0, 1]
[0.33333333333333326, -0.33333333333333337, 0.33333333333333326, -0.33333333333333337, -0.5555555555555556, 0.33333333333333326, 0.33333333333333326, 0.11111111111111116, -1.0, 1]
[0.11111111111111116, 0.5555555555555556, 0.33333333333333326, -0.11111111111111116, 0.11111111111111116, 0.5555555555555556, 0.5555555555555556, 0.7777777777777777, -0.7777777777777778, 1]
[-0.11111111111111116, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, 0.11111111111111116, 1.0, -0.5555555555555556, -1.0, -1.0, 1]
[1.0, -0.33333333333333337, -0.11111111111111116, -0.11111111111111116, -0.11111111111111116, 1.0, -0.33333333333333337, -1.0, -1.0, 1]
[-0.11111111111111116, 1.0, 1.0, -0.5555555555555556, 0.5555555555555556, -1.0, -0.11111111111111116, 1.0, -0.5555555555555556, 1]
[-0.5555555555555556, 1.0, 0.5555555555555556, 0.33333333333333326, 0.11111111111111116, 0.7777777777777777, 0.7777777777777777, -0.5555555555555556, 0.5555555555555556, 1]
[-0.11111111111111116, 0.5555555555555556, 0.33333333333333326, 0.33333333333333326, 1.0, 1.0, -0.11111111111111116, 0.33333333333333326, -1.0, 1]
[0.5555555555555556, 0.11111111111111116, -0.11111111111111116, -0.33333333333333337, -0.5555555555555556, 1.0, 0.11111111111111116, -1.0, -1.0, 1]
[-0.33333333333333337, 0.5555555555555556, 0.33333333333333326, 1.0, -0.33333333333333337, 1.0, 0.33333333333333326, -0.11111111111111116, -1.0, 1]
[0.11111111111111116, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 0.33333333333333326, 1]
[0.7777777777777777, 1.0, 1.0, 1.0, 1.0, -0.11111111111111116, 1.0, 1.0, 1.0, 1]
[0.5555555555555556, 0.33333333333333326, 0.5555555555555556, -0.11111111111111116, -0.11111111111111116, 1.0, 0.7777777777777777, 1.0, -1.0, 1]
[1.0, 0.5555555555555556, 0.5555555555555556, -0.7777777777777778, 0.5555555555555556, 1.0, -0.33333333333333337, 0.5555555555555556, 1.0, 1]
[0.7777777777777777, 0.5555555555555556, 0.5555555555555556, -0.11111111111111116, 0.11111111111111116, -0.7777777777777778, -0.33333333333333337, 1.0, -0.33333333333333337, 1]
[0.5555555555555556, 1.0, 1.0, 0.5555555555555556, 0.11111111111111116, 0.7777777777777777, -0.5555555555555556, 1.0, 1.0, 1]
[1.0, 1.0, 1.0, 1.0, 0.11111111111111116, 1.0, 0.5555555555555556, -1.0, -0.11111111111111116, 1]
[-0.5555555555555556, 0.11111111111111116, -0.33333333333333337, 1.0, -0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -0.33333333333333337, -1.0, 1]
[0.11111111111111116, -0.5555555555555556, -0.7777777777777778, -1.0, -0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -1.0, -1.0, 1]
[1.0, -0.33333333333333337, -0.5555555555555556, -0.7777777777777778, -0.5555555555555556, 1.0, -0.11111111111111116, -0.5555555555555556, -0.7777777777777778, 1]
[-0.11111111111111116, 0.5555555555555556, -0.33333333333333337, 1.0, -0.11111111111111116, 0.5555555555555556, 0.7777777777777777, 1.0, -1.0, 1]
[1.0, 1.0, 1.0, 0.5555555555555556, 0.11111111111111116, 0.5555555555555556, 0.33333333333333326, 1.0, -1.0, 1]
[0.33333333333333326, -0.11111111111111116, 1.0, 1.0, 1.0, 1.0, -0.33333333333333337, 1.0, -0.5555555555555556, 1]
[0.5555555555555556, 0.5555555555555556, 0.7777777777777777, 0.11111111111111116, 0.11111111111111116, -0.5555555555555556, 1.0, 1.0, -1.0, 1]
[0.7777777777777777, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1]
[1.0, 1.0, 1.0, 1.0, -0.5555555555555556, 1.0, 1.0, 0.11111111111111116, -1.0, 1]
[0.5555555555555556, 0.33333333333333326, 0.5555555555555556, -0.7777777777777778, -0.33333333333333337, -0.7777777777777778, -0.11111111111111116, 1.0, -1.0, 1]
[1.0, 1.0, 1.0, 1.0, 0.33333333333333326, 1.0, 0.33333333333333326, 1.0, -0.33333333333333337, 1]
[0.5555555555555556, 1.0, 0.5555555555555556, 0.5555555555555556, -0.33333333333333337, 0.5555555555555556, 0.33333333333333326, 0.33333333333333326, -1.0, 1]
[0.5555555555555556, 1.0, 1.0, 1.0, -0.11111111111111116, 1.0, 0.5555555555555556, 1.0, 0.11111111111111116, 1]
[1.0, 1.0, 1.0, 0.33333333333333326, 1.0, 1.0, 0.5555555555555556, -0.7777777777777778, -1.0, 1]
[0.11111111111111116, 1.0, 0.33333333333333326, 0.33333333333333326, 0.11111111111111116, -0.33333333333333337, 0.5555555555555556, 1.0, -0.7777777777777778, 1]
[1.0, 0.5555555555555556, 1.0, -1.0, -0.5555555555555556, 1.0, -0.11111111111111116, -1.0, -1.0, 1]
[1.0, 1.0, 1.0, -1.0, 0.11111111111111116, -1.0, -0.7777777777777778, 0.5555555555555556, -1.0, 1]
[1.0, 1.0, 0.7777777777777777, -0.5555555555555556, 0.33333333333333326, -0.11111111111111116, -0.5555555555555556, -0.11111111111111116, -1.0, 1]
[-0.11111111111111116, 1.0, 1.0, 0.7777777777777777, 0.11111111111111116, 1.0, 0.33333333333333326, 1.0, -0.11111111111111116, 1]
[1.0, -0.33333333333333337, -0.5555555555555556, 1.0, -0.33333333333333337, 1.0, 1.0, -1.0, -1.0, 1]
[-0.11111111111111116, 0.5555555555555556, 0.7777777777777777, -0.33333333333333337, -0.5555555555555556, 1.0, 0.33333333333333326, -1.0, -1.0, 1]
[1.0, 0.11111111111111116, -0.33333333333333337, -0.5555555555555556, 1.0, 1.0, 0.7777777777777777, 1.0, -1.0, 1]
[1.0, 0.5555555555555556, 0.5555555555555556, -0.33333333333333337, 1.0, 1.0, 0.5555555555555556, -1.0, -1.0, 1]
[0.7777777777777777, 0.33333333333333326, 0.33333333333333326, -0.11111111111111116, -0.11111111111111116, 1.0, 0.33333333333333326, 0.5555555555555556, -0.5555555555555556, 1]
[-0.33333333333333337, -1.0, -1.0, -0.5555555555555556, -1.0, -0.11111111111111116, -0.7777777777777778, -1.0, -1.0, 1]
[0.5555555555555556, -0.11111111111111116, 0.11111111111111116, -0.7777777777777778, -0.5555555555555556, 1.0, 0.11111111111111116, 0.11111111111111116, -1.0, 1]
[0.33333333333333326, -0.11111111111111116, 0.11111111111111116, -0.5555555555555556, -0.5555555555555556, 0.5555555555555556, 0.33333333333333326, -0.33333333333333337, -1.0, 1]
[1.0, 1.0, 1.0, 0.11111111111111116, 0.5555555555555556, -0.33333333333333337, 0.5555555555555556, -0.11111111111111116, -1.0, 1]
[-0.11111111111111116, 0.33333333333333326, 0.7777777777777777, 0.5555555555555556, 0.11111111111111116, 1.0, 0.5555555555555556, 1.0, -1.0, 1]
[1.0, -0.11111111111111116, -0.11111111111111116, 0.11111111111111116, -0.5555555555555556, 1.0, 0.33333333333333326, 0.7777777777777777, -0.7777777777777778, 1]
[1.0, -0.11111111111111116, 0.33333333333333326, -0.33333333333333337, -0.33333333333333337, 1.0, 0.5555555555555556, 0.7777777777777777, -1.0, 1]
[-0.33333333333333337, -0.11111111111111116, -0.11111111111111116, 0.5555555555555556, 0.11111111111111116, 1.0, 1.0, 0.33333333333333326, -1.0, 1]
[0.5555555555555556, 0.7777777777777777, 0.7777777777777777, -0.11111111111111116, -0.5555555555555556, -0.11111111111111116, 0.33333333333333326, 0.33333333333333326, -1.0, 1]
[1.0, -0.7777777777777778, -0.7777777777777778, -1.0, -0.7777777777777778, 0.11111111111111116, -1.0, -1.0, -0.7777777777777778, 1]
[1.0, 0.11111111111111116, -0.11111111111111116, 0.5555555555555556, -0.11111111111111116, 1.0, 0.5555555555555556, 0.11111111111111116, -1.0, 1]
[0.5555555555555556, 0.33333333333333326, 0.5555555555555556, -0.11111111111111116, 1.0, 1.0, 0.33333333333333326, -0.7777777777777778, -1.0, 1]
[1.0, 0.5555555555555556, 1.0, 1.0, 0.11111111111111116, -1.0, -0.5555555555555556, -1.0, 1.0, 1]
[-0.11111111111111116, 1.0, 1.0, 1.0, 0.11111111111111116, 1.0, 0.11111111111111116, -0.11111111111111116, -0.7777777777777778, 1]
[-0.11111111111111116, 0.11111111111111116, 0.33333333333333326, 0.5555555555555556, 0.5555555555555556, 1.0, -0.5555555555555556, 1.0, -0.5555555555555556, 1]
[-0.11111111111111116, -0.5555555555555556, -0.11111111111111116, -1.0, 0.5555555555555556, 1.0, -0.11111111111111116, -0.5555555555555556, -1.0, 1]
[1.0, 1.0, 1.0, -0.7777777777777778, 1.0, 1.0, -0.11111111111111116, -0.5555555555555556, -0.5555555555555556, 1]
[1.0, -0.33333333333333337, -0.33333333333333337, 1.0, 0.11111111111111116, 1.0, -0.11111111111111116, -0.11111111111111116, -1.0, 1]
[0.33333333333333326, 0.7777777777777777, -0.33333333333333337, 1.0, 1.0, -0.5555555555555556, -0.11111111111111116, -0.5555555555555556, -0.5555555555555556, 1]
[-0.11111111111111116, 0.5555555555555556, 0.5555555555555556, 1.0, -0.11111111111111116, 1.0, 0.5555555555555556, 1.0, -0.5555555555555556, 1]
[0.11111111111111116, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.33333333333333326, 1.0, 0.33333333333333326, 1]
[0.11111111111111116, 1.0, 1.0, 1.0, -0.33333333333333337, 1.0, 0.33333333333333326, 1.0, -1.0, 1]
[-0.33333333333333337, -0.11111111111111116, -0.11111111111111116, 1.0, -0.33333333333333337, 1.0, 0.33333333333333326, -0.11111111111111116, 0.5555555555555556, 1]
[0.33333333333333326, 0.5555555555555556, -0.5555555555555556, 0.33333333333333326, -0.33333333333333337, -0.11111111111111116, 0.33333333333333326, 0.5555555555555556, -0.7777777777777778, 1]
[1.0, 1.0, 0.11111111111111116, -0.5555555555555556, -0.5555555555555556, 1.0, -0.33333333333333337, -0.5555555555555556, -0.7777777777777778, 1]
[-0.5555555555555556, -0.5555555555555556, -0.11111111111111116, -0.7777777777777778, -0.5555555555555556, 1.0, 0.33333333333333326, -1.0, -1.0, 1]
[1.0, 1.0, 1.0, -0.5555555555555556, 1.0, 0.5555555555555556, 0.5555555555555556, -1.0, -1.0, 1]
[1.0, -0.5555555555555556, -0.5555555555555556, 1.0, -0.7777777777777778, 1.0, 0.33333333333333326, -0.5555555555555556, -0.5555555555555556, 1]
[-0.11111111111111116, -0.33333333333333337, 0.11111111111111116, 0.33333333333333326, 0.7777777777777777, 0.33333333333333326, 0.5555555555555556, 1.0, -1.0, 1]
[0.5555555555555556, 0.11111111111111116, -0.33333333333333337, -0.5555555555555556, -0.11111111111111116, 0.7777777777777777, -0.5555555555555556, -1.0, -1.0, 1]
[0.11111111111111116, -0.11111111111111116, -0.33333333333333337, -0.33333333333333337, -0.5555555555555556, 0.7777777777777777, 0.33333333333333326, 0.5555555555555556, -0.5555555555555556, 1]
[-0.5555555555555556, 1.0, -0.5555555555555556, 1.0, 0.11111111111111116, 1.0, -0.11111111111111116, -1.0, -0.33333333333333337, 1]
[-1.0, 0.11111111111111116, 0.5555555555555556, 1.0, 0.5555555555555556, 1.0, -0.11111111111111116, 0.33333333333333326, -1.0, 1]
[1.0, 1.0, 1.0, 0.5555555555555556, -0.7777777777777778, 1.0, -0.33333333333333337, -1.0, -1.0, 1]
[1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 1]
[0.5555555555555556, -0.7777777777777778, -0.5555555555555556, -1.0, 0.11111111111111116, -0.5555555555555556, 0.33333333333333326, -1.0, -1.0, 1]
[0.11111111111111116, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1]
[-0.7777777777777778, -0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -0.7777777777777778, -0.11111111111111116, -0.7777777777777778, -0.11111111111111116, -1.0, 1]
[-0.11111111111111116, 0.33333333333333326, 1.0, 1.0, -0.11111111111111116, 1.0, 1.0, 1.0, -1.0, 1]
[1.0, -0.5555555555555556, -0.11111111111111116, -1.0, 1.0, -0.11111111111111116, -0.5555555555555556, 1.0, -0.7777777777777778, 1]
[0.33333333333333326, -0.11111111111111116, 0.11111111111111116, 1.0, -0.11111111111111116, 1.0, 0.33333333333333326, 0.7777777777777777, -0.33333333333333337, 1]
[0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -1.0, 0.11111111111111116, 1.0, -0.7777777777777778, -0.11111111111111116, -0.7777777777777778, 1]
[1.0, 1.0, 0.5555555555555556, 1.0, 0.11111111111111116, -0.11111111111111116, 1.0, -0.5555555555555556, -1.0, 1]
[1.0, 0.5555555555555556, 0.5555555555555556, -0.7777777777777778, -0.5555555555555556, -0.33333333333333337, 0.5555555555555556, 0.33333333333333326, 0.5555555555555556, 1]
[0.33333333333333326, 0.11111111111111116, 1.0, -0.11111111111111116, -0.5555555555555556, 1.0, 0.7777777777777777, 1.0, -0.7777777777777778, 1]
[0.5555555555555556, 0.5555555555555556, 0.7777777777777777, -0.33333333333333337, -0.11111111111111116, 1.0, 0.33333333333333326, 0.5555555555555556, -1.0, 1]
[-0.11111111111111116, 1.0, 1.0, 0.11111111111111116, 1.0, 1.0, 1.0, 0.11111111111111116, -0.11111111111111116, 1]
[1.0, 0.5555555555555556, 0.33333333333333326, -0.33333333333333337, -0.5555555555555556, 1.0, 0.33333333333333326, 0.7777777777777777, -1.0, 1]
[1.0, 0.33333333333333326, 0.33333333333333326, -0.33333333333333337, -0.11111111111111116, 1.0, -0.11111111111111116, 0.33333333333333326, -0.7777777777777778, 1]
[0.7777777777777777, 0.7777777777777777, 1.0, -0.5555555555555556, 0.11111111111111116, 1.0, 0.33333333333333326, 1.0, 0.11111111111111116, 1]
[1.0, 1.0, 1.0, 0.33333333333333326, 0.7777777777777777, 1.0, 0.33333333333333326, 1.0, 1.0, 1]
[-0.11111111111111116, -0.11111111111111116, -0.11111111111111116, 0.11111111111111116, -0.5555555555555556, 1.0, -0.5555555555555556, -1.0, -1.0, 1]
[0.5555555555555556, 1.0, 1.0, 1.0, 0.33333333333333326, -0.11111111111111116, -0.33333333333333337, 0.5555555555555556, 0.33333333333333326, 1]
[1.0, 1.0, 0.5555555555555556, 0.11111111111111116, -0.33333333333333337, -0.11111111111111116, 0.5555555555555556, 1.0, -1.0, 1]
[0.33333333333333326, -0.7777777777777778, -0.33333333333333337, -1.0, 0.11111111111111116, 1.0, -0.11111111111111116, -0.33333333333333337, -0.5555555555555556, 1]
[0.5555555555555556, 1.0, 1.0, 0.5555555555555556, -0.11111111111111116, 1.0, 0.33333333333333326, 0.5555555555555556, -1.0, 1]
[0.5555555555555556, 0.5555555555555556, 0.33333333333333326, -0.33333333333333337, 1.0, 1.0, 0.33333333333333326, 0.5555555555555556, 0.33333333333333326, 1]
[0.33333333333333326, 0.11111111111111116, 0.11111111111111116, -0.5555555555555556, -0.7777777777777778, 1.0, 0.33333333333333326, -1.0, -1.0, 1]
[0.5555555555555556, -0.33333333333333337, -0.33333333333333337, -1.0, -0.7777777777777778, 0.7777777777777777, -0.5555555555555556, -0.5555555555555556, -1.0, 1]
[-0.5555555555555556, -0.33333333333333337, -0.11111111111111116, -0.7777777777777778, 0.11111111111111116, 0.5555555555555556, -0.33333333333333337, -1.0, -1.0, 1]
[0.33333333333333326, -0.11111111111111116, -0.5555555555555556, 0.33333333333333326, -0.33333333333333337, 1.0, 0.33333333333333326, -0.11111111111111116, -0.11111111111111116, 1]
[0.11111111111111116, 0.11111111111111116, 0.33333333333333326, 1.0, -0.5555555555555556, 1.0, 0.5555555555555556, 1.0, -0.7777777777777778, 1]
[0.7777777777777777, -0.11111111111111116, -0.11111111111111116, -0.33333333333333337, -0.33333333333333337, -0.11111111111111116, -0.33333333333333337, -0.5555555555555556, -0.5555555555555556, 1]
[-0.33333333333333337, 0.33333333333333326, 0.5555555555555556, -0.5555555555555556, -0.33333333333333337, 1.0, 0.7777777777777777, -1.0, -1.0, 1]
[1.0, -0.33333333333333337, -0.33333333333333337, 1.0, -0.7777777777777778, 1.0, -0.11111111111111116, -0.5555555555555556, -0.5555555555555556, 1]
[1.0, 0.11111111111111116, -0.5555555555555556, 0.11111111111111116, -0.33333333333333337, 1.0, 0.33333333333333326, 0.5555555555555556, -0.33333333333333337, 1]
[1.0, -0.33333333333333337, -0.11111111111111116, -0.33333333333333337, -0.5555555555555556, -0.11111111111111116, 0.33333333333333326, -0.5555555555555556, -1.0, 1]
[0.33333333333333326, -0.11111111111111116, 0.11111111111111116, 1.0, -0.33333333333333337, 1.0, -0.11111111111111116, -0.5555555555555556, -1.0, 1]
[0.11111111111111116, 1.0, 1.0, -0.7777777777777778, 0.5555555555555556, 1.0, 0.33333333333333326, -0.5555555555555556, -0.5555555555555556, 1]
[0.7777777777777777, 1.0, 1.0, -1.0, 1.0, 0.5555555555555556, -0.5555555555555556, -0.5555555555555556, -1.0, 1]
[-0.11111111111111116, 0.11111111111111116, 0.11111111111111116, -0.7777777777777778, -0.33333333333333337, 1.0, -0.5555555555555556, 0.11111111111111116, -1.0, 1]
[0.33333333333333326, -0.33333333333333337, -0.33333333333333337, -0.5555555555555556, -0.33333333333333337, 1.0, 0.11111111111111116, 0.7777777777777777, -1.0, 1]
[-0.11111111111111116, 1.0, 0.5555555555555556, 1.0, 0.5555555555555556, 1.0, -0.5555555555555556, 0.11111111111111116, -0.5555555555555556, 1]
[1.0, -0.11111111111111116, 0.5555555555555556, 1.0, -0.5555555555555556, 1.0, -0.11111111111111116, -1.0, -0.5555555555555556, 1]
[0.5555555555555556, -0.5555555555555556, -0.11111111111111116, -0.33333333333333337, -0.11111111111111116, 1.0, -1.0, 0.11111111111111116, -0.7777777777777778, 1]
[-0.33333333333333337, 1.0, -0.33333333333333337, 0.33333333333333326, -0.5555555555555556, 1.0, 0.7777777777777777, 1.0, -1.0, 1]
[-0.33333333333333337, 0.5555555555555556, 0.5555555555555556, -0.11111111111111116, -0.33333333333333337, -0.11111111111111116, 1.0, -0.33333333333333337, -1.0, 1]
the tuning data set:
     clump_thickness  cell_size_uniformity  cell_size_shape  marginal_adhesion  single_epithesial_cell_size  bare_nuclei  bland_chromatin  normal_nucleoti   mitoses result
0         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
1         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
2         -0.555556             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
3         -0.555556             -0.555556        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
4         -1.000000             -1.000000        -1.000000          -0.555556                    -1.000000    -0.555556        -1.000000        -1.000000 -1.000000      2
5         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
6         -0.555556             -0.555556        -0.777778           0.111111                    -0.555556    -0.555556        -0.555556        -0.111111 -1.000000      2
7         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
8         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
9         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
10        -1.000000             -0.777778        -0.555556          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
11        -1.000000             -0.777778        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -0.777778 -1.000000      2
12        -0.555556             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
13        -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
14        -1.000000             -0.777778        -0.777778          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
15        -0.333333             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
16        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
17        -1.000000             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
18        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
19        -0.111111             -0.777778        -0.777778          -0.777778                    -0.777778    -1.000000        -1.000000        -1.000000 -0.777778      2
20        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
21        -1.000000             -1.000000        -0.555556          -0.777778                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
22        -0.555556             -1.000000        -1.000000          -0.777778                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
23        -0.333333             -0.555556        -1.000000          -1.000000                    -0.777778    -1.000000        -0.333333         0.555556 -1.000000      2
24        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
25        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
26        -0.111111             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
27        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
28        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -0.111111        -1.000000        -1.000000 -1.000000      2
29         0.555556             -0.333333        -0.333333          -0.111111                    -0.333333     0.333333         0.333333         0.555556 -0.777778      2
..              ...                   ...              ...                ...                          ...          ...              ...              ...       ...    ...
39         0.777778              1.000000         1.000000          -1.000000                     1.000000     0.555556        -0.555556        -0.555556 -1.000000      4
40        -0.333333              0.111111         0.111111          -0.111111                     0.333333     0.111111         0.333333         0.333333 -0.555556      4
41         1.000000             -0.111111         1.000000          -0.555556                    -0.111111     0.555556         0.333333         0.555556 -0.555556      4
42         1.000000             -0.111111         0.333333          -0.555556                    -0.555556     0.333333        -0.555556        -0.555556  0.555556      4
43         0.333333              0.111111        -0.333333           0.555556                     1.000000     1.000000         0.777778        -0.111111 -0.555556      4
44        -0.111111              0.111111         0.111111           0.555556                     0.111111     1.000000        -0.333333         1.000000 -0.333333      4
45         0.333333             -0.555556        -0.777778           1.000000                    -0.111111     1.000000        -0.111111        -0.333333 -0.333333      4
46         0.555556              0.111111        -0.333333           1.000000                     1.000000    -1.000000        -0.555556        -0.111111 -1.000000      4
47        -0.111111              1.000000         1.000000          -0.555556                     0.333333    -0.555556         0.555556         1.000000 -0.777778      4
48         1.000000              1.000000         0.333333           0.555556                     0.333333    -1.000000         1.000000         1.000000 -0.555556      4
49         1.000000             -0.555556         0.111111          -0.777778                    -0.555556    -0.111111        -0.333333         1.000000 -0.777778      4
50         0.555556              0.111111         0.333333          -0.555556                    -0.555556     1.000000        -0.555556        -0.333333 -0.777778      4
51         0.111111              1.000000         1.000000          -0.777778                     0.555556     1.000000         0.333333        -0.555556 -0.555556      4
52        -1.000000             -0.111111         0.555556           0.111111                    -0.111111     0.555556         0.333333         1.000000 -1.000000      4
53         1.000000              0.777778         0.555556           0.333333                     0.111111    -0.333333         0.333333         1.000000 -0.555556      4
54         0.777778              0.555556         0.555556          -0.111111                     0.111111    -0.777778        -0.333333         1.000000 -0.333333      4
55         0.777778              1.000000         1.000000           1.000000                     1.000000     1.000000         1.000000         1.000000 -1.000000      4
56         1.000000              0.555556         1.000000          -1.000000                    -0.555556     1.000000        -0.111111        -1.000000 -1.000000      4
57         1.000000              1.000000         0.777778          -0.555556                     0.333333    -0.111111        -0.555556        -0.111111 -1.000000      4
58         1.000000             -0.111111         0.333333          -0.333333                    -0.333333     1.000000         0.555556         0.777778 -1.000000      4
59         1.000000             -0.777778        -0.777778          -1.000000                    -0.777778     0.111111        -1.000000        -1.000000 -0.777778      4
60        -0.111111              0.111111         0.333333           0.555556                     0.555556     1.000000        -0.555556         1.000000 -0.555556      4
61         1.000000              1.000000         1.000000          -0.777778                     1.000000     1.000000        -0.111111        -0.555556 -0.555556      4
62         1.000000             -0.555556        -0.555556           1.000000                    -0.777778     1.000000         0.333333        -0.555556 -0.555556      4
63        -0.111111              0.333333         1.000000           1.000000                    -0.111111     1.000000         1.000000         1.000000 -1.000000      4
64         1.000000              0.555556         0.555556          -0.777778                    -0.555556    -0.333333         0.555556         0.333333  0.555556      4
65         1.000000              1.000000         1.000000           0.333333                     0.777778     1.000000         0.333333         1.000000  1.000000      4
66         0.555556             -0.333333        -0.333333          -1.000000                    -0.777778     0.777778        -0.555556        -0.555556 -1.000000      4
67        -0.111111              1.000000         0.555556           1.000000                     0.555556     1.000000        -0.555556         0.111111 -0.555556      4
68        -0.333333              1.000000        -0.333333           0.333333                    -0.555556     1.000000         0.777778         1.000000 -1.000000      4

[69 rows x 10 columns]
group 1:
     clump_thickness  cell_size_uniformity  cell_size_shape  marginal_adhesion  single_epithesial_cell_size  bare_nuclei  bland_chromatin  normal_nucleoti   mitoses result
0          -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
1           0.555556              0.333333         0.555556          -0.777778                    -0.333333    -0.777778        -0.111111         1.000000 -1.000000      4
2           0.555556              0.333333         0.111111          -0.333333                    -0.333333     1.000000        -0.111111        -1.000000 -1.000000      4
3          -0.333333             -1.000000        -1.000000          -1.000000                    -0.555556    -1.000000        -1.000000        -1.000000 -1.000000      2
4          -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
5          -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
6          -0.111111              0.333333         1.000000           0.111111                    -0.111111     1.000000         0.333333        -0.111111 -1.000000      4
7          -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
8          -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -0.555556        -1.000000        -1.000000 -1.000000      2
9          -0.777778             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
10          0.111111             -0.555556        -0.555556          -0.111111                    -0.555556     1.000000        -0.555556        -0.111111 -0.555556      2
11         -0.555556             -0.777778        -1.000000          -1.000000                    -0.777778    -0.777778        -0.555556        -1.000000 -1.000000      2
12         -0.111111             -0.555556        -0.333333          -1.000000                    -0.333333    -1.000000        -0.555556        -1.000000 -1.000000      2
13         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
14         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
15         -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -1.000000        -0.555556 -1.000000      2
16         -0.111111              1.000000         1.000000           1.000000                    -0.111111    -0.777778         0.555556        -0.111111 -1.000000      4
17          0.777778             -0.333333        -0.111111           1.000000                     0.111111     1.000000        -0.333333         0.555556 -1.000000      4
18          0.555556              1.000000        -0.555556          -0.777778                     0.111111    -0.333333        -0.555556         1.000000 -1.000000      4
19         -0.111111             -0.111111         0.333333           0.555556                     0.111111     1.000000         0.333333        -0.333333 -1.000000      4
20         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
21          1.000000             -0.333333        -0.111111          -0.111111                    -0.111111     1.000000        -0.333333        -1.000000 -1.000000      4
22         -0.111111             -0.777778        -0.555556          -0.333333                    -0.777778     0.333333        -0.555556         0.111111 -1.000000      4
23         -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
24         -0.111111             -0.333333        -0.333333           0.777778                    -0.777778     1.000000        -0.111111         0.111111 -1.000000      4
25         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
26          0.555556              0.333333        -0.111111           1.000000                     0.333333     0.777778        -0.111111        -0.111111 -0.333333      4
27         -1.000000              0.111111         0.555556           1.000000                     0.555556     1.000000        -0.111111         0.333333 -1.000000      4
28         -0.333333             -0.777778        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
29         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
..               ...                   ...              ...                ...                          ...          ...              ...              ...       ...    ...
93         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -0.333333        -1.000000        -1.000000 -1.000000      2
94          0.333333             -1.000000        -0.777778          -0.555556                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
95         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -0.777778 -1.000000      2
96         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
97         -0.555556             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
98          0.333333             -0.333333        -0.333333          -0.555556                    -0.333333     1.000000         0.111111         0.777778 -1.000000      4
99         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -0.777778        -0.555556        -0.777778 -1.000000      2
100        -0.111111             -0.777778        -0.777778          -0.777778                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
101        -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
102        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
103         0.111111             -0.111111        -0.111111           0.555556                    -0.333333     1.000000        -0.555556        -0.333333 -1.000000      4
104         0.111111              0.111111         0.333333           1.000000                    -0.555556     1.000000         0.555556         1.000000 -0.777778      4
105        -0.333333              0.555556         0.333333           1.000000                    -0.333333     1.000000         0.333333        -0.111111 -1.000000      4
106        -0.111111             -0.777778        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
107         1.000000              0.111111        -0.333333          -0.555556                     1.000000     1.000000         0.777778         1.000000 -1.000000      4
108        -0.111111             -0.555556        -0.777778           0.555556                    -0.111111     1.000000         0.555556        -1.000000 -0.777778      4
109        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
110        -0.333333             -0.333333        -0.333333          -0.777778                    -0.777778    -0.555556        -0.777778        -1.000000 -1.000000      2
111        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
112         0.555556              0.777778         0.777778          -0.111111                    -0.555556    -0.111111         0.333333         0.333333 -1.000000      4
113        -0.111111             -0.555556        -0.333333          -0.555556                    -0.333333    -0.111111        -0.333333         0.333333 -1.000000      2
114        -0.111111             -0.777778        -0.777778          -0.333333                    -0.777778    -0.333333        -1.000000        -1.000000 -1.000000      2
115        -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
116         0.111111              1.000000         1.000000           1.000000                    -0.333333     1.000000         0.333333         1.000000 -1.000000      4
117        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
118        -0.111111             -0.555556        -0.111111          -0.111111                    -0.555556    -0.555556        -0.333333         1.000000 -1.000000      4
119         0.333333              0.111111         1.000000          -0.111111                    -0.555556     1.000000         0.777778         1.000000 -0.777778      4
120        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
121        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
122         0.777778              0.777778         1.000000          -0.555556                     0.111111     1.000000         0.333333         1.000000  0.111111      4

[123 rows x 10 columns]
group 2:
     clump_thickness  cell_size_uniformity  cell_size_shape  marginal_adhesion  single_epithesial_cell_size  bare_nuclei  bland_chromatin  normal_nucleoti   mitoses result
0           1.000000              0.555556         0.555556          -0.333333                     1.000000     1.000000         0.555556        -1.000000 -1.000000      4
1           1.000000              1.000000         1.000000           0.111111                     0.555556    -0.333333         0.555556        -0.111111 -1.000000      4
2           1.000000              1.000000         1.000000           0.555556                    -0.777778     1.000000        -0.333333        -1.000000 -1.000000      4
3           1.000000             -0.555556        -0.555556          -1.000000                    -0.777778     1.000000         0.333333         0.111111 -1.000000      4
4           0.555556              1.000000         1.000000           1.000000                     0.111111     1.000000         1.000000         1.000000 -1.000000      4
5          -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
6          -1.000000             -1.000000        -1.000000          -1.000000                    -0.333333    -0.555556        -1.000000        -1.000000 -1.000000      2
7          -0.555556             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
8          -1.000000             -1.000000        -1.000000          -0.777778                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
9          -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -1.000000        -1.000000 -1.000000      2
10         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
11         -0.333333             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
12         -0.111111             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
13          0.111111              1.000000         1.000000           1.000000                     0.555556     1.000000         1.000000         1.000000  0.333333      4
14         -0.333333             -1.000000        -0.333333          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
15          1.000000              0.111111        -0.111111           0.555556                    -0.111111     1.000000         0.555556         0.111111 -1.000000      4
16         -1.000000             -0.555556        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -0.777778 -1.000000      2
17         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
18         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
19          0.555556              0.555556         0.333333          -0.333333                     1.000000     1.000000         0.333333         0.555556  0.333333      4
20         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
21         -0.555556             -0.777778        -1.000000          -0.777778                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
22          0.333333              0.555556         0.333333           0.111111                    -0.333333    -0.555556         0.555556         0.555556 -0.333333      4
23         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
24         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -0.777778 -1.000000      2
25          1.000000              1.000000         1.000000          -1.000000                     0.111111    -1.000000        -0.777778         0.555556 -1.000000      4
26          1.000000             -0.111111         0.111111           1.000000                     0.111111     1.000000         0.333333         0.333333  1.000000      4
27         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -0.777778      2
28         -0.555556             -0.777778        -0.777778          -1.000000                    -0.777778    -1.000000        -0.777778        -0.555556 -1.000000      2
29          0.333333              0.555556         0.333333          -0.777778                    -0.333333     0.555556        -0.555556         0.555556 -0.777778      4
..               ...                   ...              ...                ...                          ...          ...              ...              ...       ...    ...
93         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -0.777778         0.333333        -1.000000 -1.000000      2
94         -0.111111             -1.000000        -0.555556          -0.555556                    -0.777778    -0.777778        -0.777778        -0.555556 -1.000000      2
95         -0.333333             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
96         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
97         -0.111111             -1.000000        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
98         -1.000000             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
99         -0.333333             -0.555556        -0.777778          -1.000000                    -0.555556    -1.000000        -0.777778        -1.000000 -1.000000      2
100         0.555556              0.333333         0.555556           0.333333                    -0.111111    -0.111111        -0.111111         1.000000 -0.777778      4
101        -0.333333             -0.111111        -0.111111           1.000000                    -0.333333     1.000000         0.333333        -0.111111  0.555556      4
102        -0.555556             -0.111111         0.333333           0.555556                     0.555556     0.777778         0.333333         1.000000  0.333333      4
103        -0.555556             -0.555556         0.111111          -0.333333                    -0.111111     0.555556        -0.333333        -0.333333 -1.000000      4
104         1.000000             -0.555556        -0.111111          -1.000000                     1.000000    -0.111111        -0.555556         1.000000 -0.777778      4
105         1.000000              1.000000         1.000000           1.000000                    -0.111111     1.000000         1.000000         1.000000  0.333333      4
106        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
107        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
108        -0.111111             -0.555556        -1.000000          -0.777778                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
109        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
110        -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
111         0.555556              1.000000         1.000000           0.333333                     1.000000     1.000000         0.333333        -0.555556  0.555556      4
112        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
113        -0.555556             -0.777778        -0.777778          -0.777778                    -0.777778    -1.000000        -0.333333        -0.777778 -1.000000      2
114        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -0.777778        -0.555556        -1.000000 -1.000000      2
115        -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
116        -0.111111             -0.333333         0.111111           0.333333                     0.777778     0.333333         0.555556         1.000000 -1.000000      4
117        -0.555556             -1.000000        -1.000000          -1.000000                    -0.555556    -1.000000        -0.777778        -1.000000 -1.000000      2
118        -0.111111             -0.111111        -0.111111           0.111111                    -0.555556     1.000000        -0.555556        -1.000000 -1.000000      4
119        -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
120        -0.111111             -1.000000        -1.000000          -0.777778                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
121        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
122        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2

[123 rows x 10 columns]
group 3:
     clump_thickness  cell_size_uniformity  cell_size_shape  marginal_adhesion  single_epithesial_cell_size  bare_nuclei  bland_chromatin  normal_nucleoti   mitoses result
0          -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
1          -0.777778             -1.000000        -0.555556          -0.777778                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
2          -0.333333             -0.777778        -0.777778          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
3          -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
4          -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
5          -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.555556        -1.000000 -1.000000      2
6          -0.333333             -1.000000        -1.000000          -0.777778                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
7          -0.111111              1.000000         1.000000           0.111111                     1.000000     1.000000         1.000000         0.111111 -0.111111      4
8          -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
9          -0.111111             -1.000000        -1.000000          -1.000000                    -0.555556    -0.777778        -0.777778        -0.777778 -1.000000      2
10         -0.111111              1.000000         1.000000           1.000000                     1.000000     1.000000         1.000000        -1.000000 -1.000000      4
11         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -0.777778 -1.000000      2
12         -0.111111             -0.777778        -0.777778          -0.777778                    -0.777778    -0.777778        -0.555556        -0.777778 -0.777778      2
13         -0.111111              1.000000         1.000000           0.777778                     0.111111     1.000000         0.333333         1.000000 -0.111111      4
14         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -0.777778        -0.777778        -1.000000 -1.000000      2
15         -0.555556             -0.555556        -0.111111          -0.777778                    -0.555556     1.000000         0.333333        -1.000000 -1.000000      4
16          0.555556              1.000000         1.000000           1.000000                     0.111111     1.000000         1.000000         1.000000  1.000000      4
17         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
18         -1.000000             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
19         -0.111111              0.333333         0.777778           0.555556                     0.111111     1.000000         0.555556         1.000000 -1.000000      4
20         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
21          1.000000              1.000000         1.000000           1.000000                     1.000000    -1.000000         0.555556         0.555556  0.555556      4
22          0.555556              1.000000         0.555556           0.555556                    -0.333333     0.555556         0.333333         0.333333 -1.000000      4
23         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
24         -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
25         -0.111111             -0.333333        -0.111111          -1.000000                     0.555556    -1.000000        -0.555556         0.111111 -1.000000      2
26         -0.111111             -0.333333        -0.333333          -0.111111                     0.333333     1.000000        -0.555556        -0.777778 -1.000000      2
27          0.333333              0.111111         0.111111          -0.555556                    -0.777778     1.000000         0.333333        -1.000000 -1.000000      4
28         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
29         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
..               ...                   ...              ...                ...                          ...          ...              ...              ...       ...    ...
93         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -0.555556 -1.000000      2
94         -0.111111             -0.555556        -0.777778          -0.333333                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
95          0.111111              1.000000         1.000000          -0.777778                     0.555556     1.000000         0.333333        -0.555556 -0.555556      4
96         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
97          0.555556             -0.777778        -0.333333          -1.000000                    -0.111111    -1.000000        -0.111111        -0.333333 -0.333333      4
98         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
99         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
100         0.333333             -0.111111         1.000000           1.000000                     1.000000     1.000000        -0.333333         1.000000 -0.555556      4
101        -1.000000             -0.555556        -1.000000          -0.777778                    -0.777778    -0.777778        -0.111111        -0.555556 -0.777778      2
102        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
103        -0.777778             -0.555556        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
104         0.555556             -0.555556        -0.333333           0.777778                    -0.555556     1.000000        -0.555556        -0.555556 -1.000000      4
105        -1.000000             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
106         0.333333              0.555556        -0.555556           0.333333                    -0.333333    -0.111111         0.333333         0.555556 -0.777778      4
107        -0.777778             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
108         0.555556             -0.555556        -0.111111          -0.333333                    -0.111111     1.000000        -1.000000         0.111111 -0.777778      4
109        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
110        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
111        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
112        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
113        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
114        -0.555556             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
115         1.000000              1.000000         1.000000          -0.555556                     1.000000     1.000000         0.777778         1.000000 -1.000000      4
116        -0.111111             -1.000000        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
117         0.555556              1.000000         1.000000           1.000000                     0.333333    -0.111111        -0.333333         0.555556  0.333333      4
118        -0.333333             -1.000000        -1.000000          -0.777778                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
119        -0.111111             -0.333333         0.111111           0.111111                    -0.333333     1.000000        -0.333333        -0.555556 -1.000000      4
120         0.111111              1.000000        -0.777778           0.555556                     1.000000    -0.777778         0.333333         0.555556  1.000000      4
121        -0.111111             -1.000000        -1.000000           0.111111                    -0.555556    -1.000000        -0.777778        -1.000000 -1.000000      2
122         0.555556              1.000000        -0.111111          -0.555556                     0.555556    -0.333333        -0.333333         1.000000 -0.555556      4

[123 rows x 10 columns]
group 4:
     clump_thickness  cell_size_uniformity  cell_size_shape  marginal_adhesion  single_epithesial_cell_size  bare_nuclei  bland_chromatin  normal_nucleoti   mitoses result
0          -0.333333             -1.000000        -1.000000          -0.555556                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
1          -0.555556             -0.333333        -0.111111          -0.555556                     0.333333    -0.555556        -0.333333         0.111111 -1.000000      2
2          -0.333333             -1.000000        -1.000000          -0.555556                    -1.000000    -0.111111        -0.777778        -1.000000 -1.000000      4
3          -0.111111             -0.555556        -0.777778          -1.000000                    -0.555556    -1.000000        -1.000000        -1.000000 -1.000000      2
4           0.111111              0.555556         0.333333          -0.111111                     0.111111     0.555556         0.555556         0.777778 -0.777778      4
5          -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
6          -0.555556             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -1.000000        -1.000000 -1.000000      2
7          -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
8          -1.000000             -0.777778        -0.777778          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
9          -0.333333              0.555556         0.555556          -0.111111                    -0.333333    -0.111111         1.000000        -0.333333 -1.000000      4
10          0.111111             -1.000000        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
11         -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
12         -0.111111             -1.000000        -0.333333          -1.000000                    -0.777778    -1.000000        -0.555556        -0.777778 -1.000000      2
13         -0.555556              0.333333         0.333333          -0.333333                    -0.333333     0.777778        -0.333333         0.555556 -1.000000      4
14         -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.555556        -1.000000 -1.000000      2
15         -0.111111             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
16         -0.777778             -0.555556        -1.000000          -1.000000                    -0.111111    -1.000000        -1.000000        -1.000000 -1.000000      2
17         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
18          0.111111             -0.555556        -0.555556          -0.555556                    -0.555556    -0.777778         0.111111        -1.000000 -1.000000      2
19         -0.333333             -0.777778        -1.000000          -1.000000                    -0.777778    -0.777778        -0.555556        -1.000000 -1.000000      2
20         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
21         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
22         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
23         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -0.555556        -0.555556        -1.000000 -1.000000      2
24          0.555556             -0.333333         0.333333          -1.000000                    -0.555556     1.000000        -0.555556         0.777778 -0.777778      4
25         -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
26         -1.000000             -0.777778        -0.555556          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
27          0.555556              0.555556         0.777778           0.111111                     0.111111    -0.555556         1.000000         1.000000 -1.000000      4
28         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
29         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -0.111111        -1.000000        -1.000000 -1.000000      2
..               ...                   ...              ...                ...                          ...          ...              ...              ...       ...    ...
93         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
94         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
95         -0.111111              0.555556         0.333333           0.333333                     1.000000     1.000000        -0.111111         0.333333 -1.000000      4
96         -1.000000             -1.000000        -1.000000          -0.777778                    -1.000000    -1.000000        -1.000000        -1.000000 -1.000000      2
97         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
98          1.000000              1.000000         0.555556           0.111111                    -0.333333    -0.111111         0.555556         1.000000 -1.000000      4
99          0.777778              0.555556         0.555556           0.777778                     0.111111    -0.555556        -0.333333        -1.000000 -1.000000      4
100        -0.555556             -0.555556        -0.777778          -1.000000                    -0.777778    -0.555556        -0.555556        -1.000000 -1.000000      2
101        -0.111111             -0.111111        -0.111111           0.555556                     1.000000     0.555556         0.333333        -0.555556  0.333333      4
102        -0.111111             -0.111111        -0.111111          -0.777778                    -0.111111     1.000000        -0.333333        -0.555556 -1.000000      4
103        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
104         1.000000              0.555556         1.000000           1.000000                     0.111111    -1.000000        -0.555556        -1.000000  1.000000      4
105         0.555556              1.000000         1.000000          -1.000000                    -0.555556     0.111111        -0.555556         0.777778 -1.000000      4
106         0.555556             -0.333333         0.111111          -0.555556                    -0.555556    -1.000000        -0.333333        -0.555556 -1.000000      2
107        -0.555556             -1.000000        -0.777778          -0.777778                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
108        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
109         0.111111             -0.555556        -0.777778          -1.000000                    -0.555556    -0.333333        -0.333333        -1.000000 -1.000000      4
110        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -0.777778 -1.000000      2
111        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
112        -0.555556             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
113        -0.333333             -1.000000        -1.000000          -1.000000                    -0.555556    -1.000000        -0.777778        -0.777778 -1.000000      2
114         0.555556              0.333333         0.555556          -0.111111                    -0.111111     1.000000         0.777778         1.000000 -1.000000      4
115        -0.333333             -0.333333        -0.777778          -1.000000                    -0.777778    -0.111111        -0.777778        -1.000000 -0.777778      2
116        -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.555556        -1.000000 -1.000000      2
117        -1.000000             -1.000000        -0.777778          -1.000000                    -0.777778    -0.777778        -0.333333        -0.777778 -1.000000      2
118        -0.333333             -1.000000        -0.555556          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
119        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000  0.555556      2
120        -1.000000             -1.000000        -1.000000          -1.000000                    -0.555556    -0.777778        -0.777778        -1.000000 -1.000000      2
121        -0.111111             -0.555556        -0.555556          -0.555556                    -0.777778    -0.555556        -0.333333        -0.333333 -1.000000      4
122        -0.111111              1.000000         1.000000           0.555556                    -0.111111    -0.111111         0.333333         1.000000 -1.000000      4

[123 rows x 10 columns]
group 5:
     clump_thickness  cell_size_uniformity  cell_size_shape  marginal_adhesion  single_epithesial_cell_size  bare_nuclei  bland_chromatin  normal_nucleoti   mitoses result
0          -1.000000             -1.000000        -0.555556          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
1          -0.111111              1.000000         0.111111          -1.000000                     1.000000    -0.333333        -0.333333         1.000000  1.000000      4
2          -0.111111             -0.777778        -0.777778          -0.777778                    -0.555556    -1.000000        -1.000000        -0.555556 -1.000000      2
3          -0.333333              0.555556         0.111111          -0.333333                    -0.555556    -0.333333         1.000000         0.111111 -1.000000      4
4          -0.111111             -0.555556        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
5          -0.555556             -0.555556        -0.111111          -0.777778                    -0.555556     1.000000         0.333333        -1.000000 -1.000000      4
6           0.111111              1.000000         1.000000           1.000000                     1.000000     1.000000         0.555556         1.000000  1.000000      4
7          -0.111111             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
8           1.000000             -0.333333        -0.555556           1.000000                    -0.555556     1.000000         0.333333        -1.000000 -0.777778      4
9           0.111111             -1.000000        -0.555556          -1.000000                    -0.333333    -0.111111        -0.111111         1.000000 -1.000000      4
10         -0.555556             -0.555556        -0.777778          -0.777778                    -0.555556    -1.000000        -1.000000        -0.777778 -0.555556      2
11          0.333333             -0.111111        -0.555556           0.333333                    -0.333333     1.000000         0.333333        -0.111111 -0.111111      4
12         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -0.555556 -1.000000      2
13          1.000000              0.111111        -0.555556           0.111111                    -0.333333     1.000000         0.333333         0.555556 -0.333333      4
14         -1.000000             -1.000000        -0.555556          -1.000000                    -1.000000    -1.000000        -0.777778        -1.000000 -1.000000      2
15         -0.333333             -1.000000        -1.000000          -0.777778                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
16         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556         0.111111 -1.000000      2
17         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
18          1.000000             -0.333333         0.111111          -0.333333                    -0.111111     1.000000         0.333333        -1.000000 -1.000000      4
19         -0.111111             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
20         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
21         -0.555556              1.000000         0.333333           0.555556                    -0.111111     0.555556         0.333333        -0.333333 -1.000000      4
22          0.111111              0.111111         0.111111          -0.111111                    -0.333333     1.000000         0.333333         0.111111 -0.777778      4
23          1.000000              0.555556         0.555556          -0.777778                     0.555556     1.000000        -0.333333         0.555556  1.000000      4
24         -1.000000             -1.000000        -0.777778          -0.777778                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
25         -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
26         -0.555556             -1.000000        -1.000000          -0.333333                    -0.555556    -1.000000        -0.777778        -0.777778 -1.000000      2
27         -0.777778             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
28          0.333333             -0.777778        -0.333333          -1.000000                     0.111111     1.000000        -0.111111        -0.333333 -0.555556      4
29         -1.000000             -1.000000        -1.000000          -0.777778                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
..               ...                   ...              ...                ...                          ...          ...              ...              ...       ...    ...
92         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
93          0.555556             -0.777778        -1.000000          -1.000000                    -0.111111    -1.000000        -1.000000        -1.000000 -1.000000      2
94          0.555556              0.111111        -0.111111          -0.333333                    -0.555556     1.000000         0.111111        -1.000000 -1.000000      4
95         -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -0.777778      2
96         -1.000000             -1.000000        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
97         -0.111111             -1.000000        -1.000000          -0.555556                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
98          1.000000              0.333333         0.333333          -0.333333                    -0.111111     1.000000        -0.111111         0.333333 -0.777778      4
99          1.000000              1.000000         1.000000           1.000000                    -0.555556     1.000000         1.000000         0.111111 -1.000000      4
100        -0.555556              1.000000        -0.555556           1.000000                     0.111111     1.000000        -0.111111        -1.000000 -0.333333      4
101        -0.111111             -1.000000        -0.777778          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
102        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
103        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -0.777778 -1.000000      2
104        -1.000000             -1.000000        -1.000000          -1.000000                    -0.111111    -1.000000        -0.555556        -1.000000 -1.000000      2
105        -0.111111              0.111111         0.111111          -0.777778                    -0.333333     1.000000        -0.555556         0.111111 -1.000000      4
106        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
107        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
108        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
109        -0.111111             -1.000000        -1.000000          -0.333333                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
110        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
111        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
112         1.000000              1.000000         1.000000           0.333333                     1.000000     1.000000         0.555556        -0.777778 -1.000000      4
113        -0.111111             -0.555556         0.111111          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
114        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2
115        -0.555556             -0.777778        -0.777778          -1.000000                    -0.333333    -0.555556        -0.777778        -1.000000 -1.000000      2
116        -1.000000             -1.000000        -1.000000          -1.000000                    -1.000000    -1.000000        -0.555556        -1.000000 -1.000000      2
117        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
118        -0.333333             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.777778        -1.000000 -1.000000      2
119         0.555556             -0.111111         0.111111          -0.777778                    -0.555556     1.000000         0.111111         0.111111 -1.000000      4
120        -0.555556             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -0.555556        -1.000000 -1.000000      2
121        -1.000000             -1.000000        -1.000000          -1.000000                    -0.777778    -1.000000        -1.000000        -1.000000 -1.000000      2

[122 rows x 10 columns]
l_rate: 10; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 10.000, error: 51.64669832
epoch: 100, lrate: 10.000, error: 16.32135105
epoch: 200, lrate: 10.000, error: 16.45588913
epoch: 300, lrate: 10.000, error: 16.00306685
epoch: 400, lrate: 10.000, error: 13.97643904
epoch: 500, lrate: 10.000, error: 13.48001921
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-36.77503766524896, -15.313967259946033, 15.040853903129113, -22.486716887842476, -12.706867112291397, -35.93596249744838, -11.619340057575547, -12.568126531362042, -6.955908920504219, -59.21301634594869], 'output': 1.0, 'delta': 0.0}, {'weights': [31.189358768166056, 5.126496734323753, 1.4608243021326, 31.012158675943926, -1.9072376802744846, 29.761486749570544, 3.46090763237815, 12.618669364474112, 10.4600668518472, 59.55650761388568], 'output': 1.5255091505347107e-28, 'delta': -2.327178168365135e-56}]
node #0: {'weights': [-36.77503766524896, -15.313967259946033, 15.040853903129113, -22.486716887842476, -12.706867112291397, -35.93596249744838, -11.619340057575547, -12.568126531362042, -6.955908920504219, -59.21301634594869], 'output': 1.0, 'delta': 0.0}
node #1: {'weights': [31.189358768166056, 5.126496734323753, 1.4608243021326, 31.012158675943926, -1.9072376802744846, 29.761486749570544, 3.46090763237815, 12.618669364474112, 10.4600668518472, 59.55650761388568], 'output': 1.5255091505347107e-28, 'delta': -2.327178168365135e-56}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 10.000, error: 50.51822111
epoch: 100, lrate: 10.000, error: 18.15889705
epoch: 200, lrate: 10.000, error: 18.16711157
epoch: 300, lrate: 10.000, error: 14.64077306
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-32.99263308244741, -7.131374665338586, -5.213280901173394, -22.256542396399606, 0.5087416370917169, -29.187810396333077, -4.650846524128049, -11.196305699533005, -2.8846379158736473, -50.53526907599364], 'output': 1.0, 'delta': 0.0}, {'weights': [25.400380028479688, 4.84279863474445, 0.08804187360615096, 24.77587361505459, -0.33853458694334115, 24.405302414743172, 3.3576997162893347, 10.751509900414302, 8.172850166492816, 48.686758614762965], 'output': 1.121945332369597e-23, 'delta': -1.2587613288259254e-46}]
node #0: {'weights': [-32.99263308244741, -7.131374665338586, -5.213280901173394, -22.256542396399606, 0.5087416370917169, -29.187810396333077, -4.650846524128049, -11.196305699533005, -2.8846379158736473, -50.53526907599364], 'output': 1.0, 'delta': 0.0}
node #1: {'weights': [25.400380028479688, 4.84279863474445, 0.08804187360615096, 24.77587361505459, -0.33853458694334115, 24.405302414743172, 3.3576997162893347, 10.751509900414302, 8.172850166492816, 48.686758614762965], 'output': 1.121945332369597e-23, 'delta': -1.2587613288259254e-46}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.500, error: 168.60478358
epoch: 100, lrate: 0.500, error: 27.26812141
epoch: 200, lrate: 0.500, error: 24.27613926
epoch: 300, lrate: 0.500, error: 22.72730626
epoch: 400, lrate: 0.500, error: 21.68132432
epoch: 500, lrate: 0.500, error: 20.91024106
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-6.268469041682991, -3.0501847730023983, 2.2325243402711985, -4.426926842666992, -0.7863833941962561, -5.740133776958536, -1.5605765973322097, -2.6271901671521163, -4.997729689503125, -14.45808543235606], 'output': 0.9999966017026892, 'delta': 1.1548385367439615e-11}, {'weights': [6.27165153088741, 3.0516889144326655, -2.2344969040364506, 4.4293132589507564, 0.7869655618685548, 5.743041284231741, 1.561348737370343, 2.6286639863032053, 5.002651848672196, 14.467378591819276], 'output': 3.37683469613009e-06, 'delta': -1.1402974058899528e-11}]
node #0: {'weights': [-6.268469041682991, -3.0501847730023983, 2.2325243402711985, -4.426926842666992, -0.7863833941962561, -5.740133776958536, -1.5605765973322097, -2.6271901671521163, -4.997729689503125, -14.45808543235606], 'output': 0.9999966017026892, 'delta': 1.1548385367439615e-11}
node #1: {'weights': [6.27165153088741, 3.0516889144326655, -2.2344969040364506, 4.4293132589507564, 0.7869655618685548, 5.743041284231741, 1.561348737370343, 2.6286639863032053, 5.002651848672196, 14.467378591819276], 'output': 3.37683469613009e-06, 'delta': -1.1402974058899528e-11}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.500, error: 118.84075964
epoch: 100, lrate: 0.500, error: 27.26732104
epoch: 200, lrate: 0.500, error: 24.27636721
epoch: 300, lrate: 0.500, error: 22.72928097
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-5.3222886947204655, -2.4227459385828234, 1.469944721202898, -3.7348366557709314, -0.6245456566435311, -4.8849841376040475, -1.3781469011838137, -2.1617712531318842, -3.6456510921676135, -11.817377898197634], 'output': 0.9999785305862962, 'delta': 4.6092582876708726e-10}, {'weights': [5.3256232964122745, 2.4225891107492092, -1.4701649788588182, 3.7379727341438223, 0.6252194108529869, 4.88828859115693, 1.3791473662838025, 2.163261190126169, 3.6599715050173796, 11.836709746129547], 'output': 2.1311107505263667e-05, 'delta': -4.5415362437792807e-10}]
node #0: {'weights': [-5.3222886947204655, -2.4227459385828234, 1.469944721202898, -3.7348366557709314, -0.6245456566435311, -4.8849841376040475, -1.3781469011838137, -2.1617712531318842, -3.6456510921676135, -11.817377898197634], 'output': 0.9999785305862962, 'delta': 4.6092582876708726e-10}
node #1: {'weights': [5.3256232964122745, 2.4225891107492092, -1.4701649788588182, 3.7379727341438223, 0.6252194108529869, 4.88828859115693, 1.3791473662838025, 2.163261190126169, 3.6599715050173796, 11.836709746129547], 'output': 2.1311107505263667e-05, 'delta': -4.5415362437792807e-10}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.100, error: 185.69603274
epoch: 100, lrate: 0.100, error: 38.87614940
epoch: 200, lrate: 0.100, error: 32.90996125
epoch: 300, lrate: 0.100, error: 30.12881530
epoch: 400, lrate: 0.100, error: 28.43273066
epoch: 500, lrate: 0.100, error: 27.24955470
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-3.987576042973025, -1.4778280355348752, 0.2548951945657708, -2.754062932055314, -0.407190986217121, -3.7077993114819314, -1.0604249925392923, -1.502650970586342, -1.3993726783108367, -7.649083977910338], 'output': 0.9997521502179623, 'delta': 6.141428916438388e-08}, {'weights': [3.998698629372109, 1.4886438426902424, -0.26571465772715347, 2.767487361424877, 0.40846886507858626, 3.7225014717706255, 1.0584950553951178, 1.5087208522355944, 1.4603592617617212, 7.729558278491293], 'output': 0.0002417580136398965, 'delta': -5.843280714367746e-08}]
node #0: {'weights': [-3.987576042973025, -1.4778280355348752, 0.2548951945657708, -2.754062932055314, -0.407190986217121, -3.7077993114819314, -1.0604249925392923, -1.502650970586342, -1.3993726783108367, -7.649083977910338], 'output': 0.9997521502179623, 'delta': 6.141428916438388e-08}
node #1: {'weights': [3.998698629372109, 1.4886438426902424, -0.26571465772715347, 2.767487361424877, 0.40846886507858626, 3.7225014717706255, 1.0584950553951178, 1.5087208522355944, 1.4603592617617212, 7.729558278491293], 'output': 0.0002417580136398965, 'delta': -5.843280714367746e-08}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.100, error: 207.01993471
epoch: 100, lrate: 0.100, error: 38.99579695
epoch: 200, lrate: 0.100, error: 32.95406633
epoch: 300, lrate: 0.100, error: 30.15880334
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-3.5587590450359383, -1.2691687288982576, -0.015985851367847734, -2.3933783199716907, -0.3680100944173173, -3.2780800692177046, -1.0757488227899423, -1.3226793061391837, -0.738811378264421, -6.402598532538042], 'output': 0.9994668864118298, 'delta': 2.8405858162663366e-07}, {'weights': [3.493311252049152, 1.1290000712239574, 0.1766649822001022, 2.388839597880582, 0.3496604877393837, 3.244792005041504, 1.0946995280393317, 1.2965570772716417, 0.8537893104553853, 6.478429502821658], 'output': 0.0005689310772816951, 'delta': -3.234984176232662e-07}]
node #0: {'weights': [-3.5587590450359383, -1.2691687288982576, -0.015985851367847734, -2.3933783199716907, -0.3680100944173173, -3.2780800692177046, -1.0757488227899423, -1.3226793061391837, -0.738811378264421, -6.402598532538042], 'output': 0.9994668864118298, 'delta': 2.8405858162663366e-07}
node #1: {'weights': [3.493311252049152, 1.1290000712239574, 0.1766649822001022, 2.388839597880582, 0.3496604877393837, 3.244792005041504, 1.0946995280393317, 1.2965570772716417, 0.8537893104553853, 6.478429502821658], 'output': 0.0005689310772816951, 'delta': -3.234984176232662e-07}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
The tuned model looks like: l_rate: 10; n_epoch: 500, n_hidden: []
The model has a loss of 2.898550724637681
epoch: 0, lrate: 10.000, error: 52.86109583
epoch: 100, lrate: 10.000, error: 16.60624437
epoch: 200, lrate: 10.000, error: 15.86963926
epoch: 300, lrate: 10.000, error: 15.83871704
epoch: 400, lrate: 10.000, error: 15.01215242
epoch: 500, lrate: 10.000, error: 15.19059170
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-22.911353403346443, -7.236076361837856, 4.695762974524905, -21.76794410577322, -0.5607924173433029, -21.913918895205033, -3.978990803039617, -10.616316697851909, -10.085754458890994, -46.615003304996186], 'output': 1.0, 'delta': 0.0}, {'weights': [37.1471077101737, 15.506044461553598, -14.722945937966005, 24.092671634035106, 13.723676020532281, 36.0500296618855, 12.156293337455022, 13.48418169737357, 7.761080051534121, 58.95857455804473], 'output': 7.431963903432163e-37, 'delta': -5.5234087461918635e-73}]
node #0: {'weights': [-22.911353403346443, -7.236076361837856, 4.695762974524905, -21.76794410577322, -0.5607924173433029, -21.913918895205033, -3.978990803039617, -10.616316697851909, -10.085754458890994, -46.615003304996186], 'output': 1.0, 'delta': 0.0}
node #1: {'weights': [37.1471077101737, 15.506044461553598, -14.722945937966005, 24.092671634035106, 13.723676020532281, 36.0500296618855, 12.156293337455022, 13.48418169737357, 7.761080051534121, 58.95857455804473], 'output': 7.431963903432163e-37, 'delta': -5.5234087461918635e-73}
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 10.000, error: 63.55128049
epoch: 100, lrate: 10.000, error: 23.84209682
epoch: 200, lrate: 10.000, error: 22.52345574
epoch: 300, lrate: 10.000, error: 22.22873041
epoch: 400, lrate: 10.000, error: 20.39430813
epoch: 500, lrate: 10.000, error: 20.00054970
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-45.09836914464151, -11.318434447828215, -15.924274745009479, -1.90081448892862, 16.04838064193811, -30.744014756693634, 4.457657810251289, -26.66740038890361, -0.21056996477991258, -44.93542562497552], 'output': 1.0, 'delta': 0.0}, {'weights': [45.35605614017928, 11.307248452275077, 16.145879690640996, 1.8510455448602792, -16.32977590015198, 30.934943560589147, -4.45935113297467, 26.90941066406294, -0.053801808008117895, 44.97202358348133], 'output': 2.8911870620399143e-31, 'delta': -8.358962627706991e-62}]
node #0: {'weights': [-45.09836914464151, -11.318434447828215, -15.924274745009479, -1.90081448892862, 16.04838064193811, -30.744014756693634, 4.457657810251289, -26.66740038890361, -0.21056996477991258, -44.93542562497552], 'output': 1.0, 'delta': 0.0}
node #1: {'weights': [45.35605614017928, 11.307248452275077, 16.145879690640996, 1.8510455448602792, -16.32977590015198, 30.934943560589147, -4.45935113297467, 26.90941066406294, -0.053801808008117895, 44.97202358348133], 'output': 2.8911870620399143e-31, 'delta': -8.358962627706991e-62}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 10.000, error: 60.10651093
epoch: 100, lrate: 10.000, error: 22.86640634
epoch: 200, lrate: 10.000, error: 22.55736741
epoch: 300, lrate: 10.000, error: 23.35395867
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-39.572220475223226, -11.016521464803194, -15.729161010730298, -2.1491016615150627, 12.712432566460194, -25.46365924403102, 1.8394895078128033, -21.52929322583616, 0.4203377874167204, -40.08286142776078], 'output': 1.0, 'delta': 0.0}, {'weights': [42.7579709308132, 12.184267065343072, 14.687889135671004, 2.0332078399709834, -11.901926551626806, 26.56004681557867, -3.7250722475573363, 22.864700988923772, -2.3381199479417005, 40.79743073235625], 'output': 6.077307989115024e-29, 'delta': -3.6933672394561303e-57}]
node #0: {'weights': [-39.572220475223226, -11.016521464803194, -15.729161010730298, -2.1491016615150627, 12.712432566460194, -25.46365924403102, 1.8394895078128033, -21.52929322583616, 0.4203377874167204, -40.08286142776078], 'output': 1.0, 'delta': 0.0}
node #1: {'weights': [42.7579709308132, 12.184267065343072, 14.687889135671004, 2.0332078399709834, -11.901926551626806, 26.56004681557867, -3.7250722475573363, 22.864700988923772, -2.3381199479417005, 40.79743073235625], 'output': 6.077307989115024e-29, 'delta': -3.6933672394561303e-57}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.500, error: 139.27699030
epoch: 100, lrate: 0.500, error: 41.02603825
epoch: 200, lrate: 0.500, error: 37.77235848
epoch: 300, lrate: 0.500, error: 35.97670975
epoch: 400, lrate: 0.500, error: 34.72568738
epoch: 500, lrate: 0.500, error: 33.77960333
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-3.23514644933549, -1.584949326365154, -2.1622871333960223, -0.96289985090921, 1.6659982822251345, -3.5398856843055158, -2.6835842191621633, -2.571289205628988, -4.1273921026297025, -10.633331245353183], 'output': 0.9998687566669883, 'delta': 1.7222551818230734e-08}, {'weights': [3.2428661955107194, 1.5850135171463091, 2.1701831050853166, 0.9643534723106038, -1.6714833361753811, 3.549203027138344, 2.6905292720235106, 2.5775544165476214, 4.142119266974872, 10.660773420197803], 'output': 0.0001283006156878868, 'delta': -1.6458936023299367e-08}]
node #0: {'weights': [-3.23514644933549, -1.584949326365154, -2.1622871333960223, -0.96289985090921, 1.6659982822251345, -3.5398856843055158, -2.6835842191621633, -2.571289205628988, -4.1273921026297025, -10.633331245353183], 'output': 0.9998687566669883, 'delta': 1.7222551818230734e-08}
node #1: {'weights': [3.2428661955107194, 1.5850135171463091, 2.1701831050853166, 0.9643534723106038, -1.6714833361753811, 3.549203027138344, 2.6905292720235106, 2.5775544165476214, 4.142119266974872, 10.660773420197803], 'output': 0.0001283006156878868, 'delta': -1.6458936023299367e-08}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.500, error: 138.16955975
epoch: 100, lrate: 0.500, error: 41.07456755
epoch: 200, lrate: 0.500, error: 37.80624216
epoch: 300, lrate: 0.500, error: 36.00220660
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.898777862402981, -1.5619860421943743, -1.8254015856293577, -0.8889137801421654, 1.4204703971222994, -3.120210746890365, -2.3478835017450965, -2.2801872396835368, -3.3861604017497884, -9.294064255501123], 'output': 0.9996333169477443, 'delta': 1.344071579061355e-07}, {'weights': [2.8977294802487474, 1.5525144145320586, 1.8431895107637968, 0.8918736993371156, -1.4274924043544224, 3.12285308617711, 2.3576131741410453, 2.282164780518241, 3.406679546633, 9.322199941999425], 'output': 0.00036249212526174673, 'delta': -1.313529092154549e-07}]
node #0: {'weights': [-2.898777862402981, -1.5619860421943743, -1.8254015856293577, -0.8889137801421654, 1.4204703971222994, -3.120210746890365, -2.3478835017450965, -2.2801872396835368, -3.3861604017497884, -9.294064255501123], 'output': 0.9996333169477443, 'delta': 1.344071579061355e-07}
node #1: {'weights': [2.8977294802487474, 1.5525144145320586, 1.8431895107637968, 0.8918736993371156, -1.4274924043544224, 3.12285308617711, 2.3576131741410453, 2.282164780518241, 3.406679546633, 9.322199941999425], 'output': 0.00036249212526174673, 'delta': -1.313529092154549e-07}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.100, error: 232.00404007
epoch: 100, lrate: 0.100, error: 52.95292145
epoch: 200, lrate: 0.100, error: 47.65904025
epoch: 300, lrate: 0.100, error: 45.20326463
epoch: 400, lrate: 0.100, error: 43.73512733
epoch: 500, lrate: 0.100, error: 42.73352702
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.4897172893344424, -1.2093770417185563, -1.3411422750155948, -0.8783888166601078, 1.1090409851562368, -2.567962724685562, -1.2902972892364728, -1.9030771093816248, -1.9182965371930667, -6.292584806275197], 'output': 0.9984110385432987, 'delta': 2.52078670336255e-06}, {'weights': [2.4821042460376628, 1.2055879356669439, 1.345336778024884, 0.8830904248540594, -1.1074779648603972, 2.5652229077380646, 1.2961524300031404, 1.9004229899710188, 1.951386902426944, 6.32690596728837], 'output': 0.0015922328056110155, 'delta': -2.531168670204741e-06}]
node #0: {'weights': [-2.4897172893344424, -1.2093770417185563, -1.3411422750155948, -0.8783888166601078, 1.1090409851562368, -2.567962724685562, -1.2902972892364728, -1.9030771093816248, -1.9182965371930667, -6.292584806275197], 'output': 0.9984110385432987, 'delta': 2.52078670336255e-06}
node #1: {'weights': [2.4821042460376628, 1.2055879356669439, 1.345336778024884, 0.8830904248540594, -1.1074779648603972, 2.5652229077380646, 1.2961524300031404, 1.9004229899710188, 1.951386902426944, 6.32690596728837], 'output': 0.0015922328056110155, 'delta': -2.531168670204741e-06}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.100, error: 298.41006243
epoch: 100, lrate: 0.100, error: 53.09797511
epoch: 200, lrate: 0.100, error: 47.72802841
epoch: 300, lrate: 0.100, error: 45.24034431
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.411707488188753, -1.1693640627144841, -1.1810530997003417, -0.8099270269427381, 0.9691644653282373, -2.440630094809969, -1.1898866794818976, -1.789339111614638, -1.333483703894689, -5.445222310975827], 'output': 0.9978202649206354, 'delta': 4.7408885607802865e-06}, {'weights': [2.3703056895512424, 1.1066869680868423, 1.2333777524045315, 0.8260239443036662, -0.9755530900616386, 2.4222796222865877, 1.2215008314091504, 1.7794870112472312, 1.4072750607357416, 5.507369159849167], 'output': 0.00223606426742804, 'delta': -4.988803123831982e-06}]
node #0: {'weights': [-2.411707488188753, -1.1693640627144841, -1.1810530997003417, -0.8099270269427381, 0.9691644653282373, -2.440630094809969, -1.1898866794818976, -1.789339111614638, -1.333483703894689, -5.445222310975827], 'output': 0.9978202649206354, 'delta': 4.7408885607802865e-06}
node #1: {'weights': [2.3703056895512424, 1.1066869680868423, 1.2333777524045315, 0.8260239443036662, -0.9755530900616386, 2.4222796222865877, 1.2215008314091504, 1.7794870112472312, 1.4072750607357416, 5.507369159849167], 'output': 0.00223606426742804, 'delta': -4.988803123831982e-06}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
The tuned model looks like: l_rate: 0.5; n_epoch: 500, n_hidden: []
The model has a loss of 1.4492753623188406
epoch: 0, lrate: 0.500, error: 144.19009393
epoch: 100, lrate: 0.500, error: 41.04807313
epoch: 200, lrate: 0.500, error: 37.79404695
epoch: 300, lrate: 0.500, error: 35.99368912
epoch: 400, lrate: 0.500, error: 34.73866705
epoch: 500, lrate: 0.500, error: 33.78979829
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-3.235099540421157, -1.5862955914993813, -2.1596570017996726, -0.9626142767218013, 1.664931367990132, -3.539508252097853, -2.6825381164378244, -2.5711154417099418, -4.125419437585513, -10.630523420236896], 'output': 0.9998685532841174, 'delta': 1.7275967948535646e-08}, {'weights': [3.238879742111769, 1.5832194629513372, 2.1694732366948117, 0.9639541670284233, -1.6700001384111827, 3.544797001971631, 2.688225043999532, 2.574484565460962, 4.136906629776329, 10.64995354410366], 'output': 0.00012957809411620838, 'delta': -1.6788306796070584e-08}]
node #0: {'weights': [-3.235099540421157, -1.5862955914993813, -2.1596570017996726, -0.9626142767218013, 1.664931367990132, -3.539508252097853, -2.6825381164378244, -2.5711154417099418, -4.125419437585513, -10.630523420236896], 'output': 0.9998685532841174, 'delta': 1.7275967948535646e-08}
node #1: {'weights': [3.238879742111769, 1.5832194629513372, 2.1694732366948117, 0.9639541670284233, -1.6700001384111827, 3.544797001971631, 2.688225043999532, 2.574484565460962, 4.136906629776329, 10.64995354410366], 'output': 0.00012957809411620838, 'delta': -1.6788306796070584e-08}
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
l_rate: 10; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 10.000, error: 69.35744209
epoch: 100, lrate: 10.000, error: 22.82760334
epoch: 200, lrate: 10.000, error: 20.74634933
epoch: 300, lrate: 10.000, error: 21.13682148
epoch: 400, lrate: 10.000, error: 19.34720620
epoch: 500, lrate: 10.000, error: 19.12353254
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-33.94252214318436, -9.54730328062442, -19.17909323854398, 8.598484062587406, 17.600340813419578, -24.890864595535522, -3.0574479584036474, -22.459044007871828, 0.11611812644710238, -31.846208518733995], 'output': 1.0, 'delta': 0.0}, {'weights': [15.995738320249117, 16.77695060534121, 16.252215854224968, -1.7018923207071768, 24.796955049681966, 21.809852840582288, 3.888014613772077, 18.134512657825212, -0.5613191923603189, 61.023461861640016], 'output': 6.04807436368548e-22, 'delta': -3.657920350866953e-43}]
node #0: {'weights': [-33.94252214318436, -9.54730328062442, -19.17909323854398, 8.598484062587406, 17.600340813419578, -24.890864595535522, -3.0574479584036474, -22.459044007871828, 0.11611812644710238, -31.846208518733995], 'output': 1.0, 'delta': 0.0}
node #1: {'weights': [15.995738320249117, 16.77695060534121, 16.252215854224968, -1.7018923207071768, 24.796955049681966, 21.809852840582288, 3.888014613772077, 18.134512657825212, -0.5613191923603189, 61.023461861640016], 'output': 6.04807436368548e-22, 'delta': -3.657920350866953e-43}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 10.000, error: 58.19854407
epoch: 100, lrate: 10.000, error: 24.19107735
epoch: 200, lrate: 10.000, error: 22.87406759
epoch: 300, lrate: 10.000, error: 21.30713304
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-14.901706013793822, -9.566077118431668, -15.843660282889324, -0.47251888954627835, -16.539356868656355, -14.759586870933012, -6.323594556719743, -17.24367000191697, -4.039609552936097, -54.0171167911726], 'output': 1.0, 'delta': 0.0}, {'weights': [31.38219642990017, 8.434499927579063, 18.17271544984127, -7.789699522381183, -16.369254379338013, 23.138328981397684, 3.0960515403605795, 20.69813457437844, 0.33000390101636634, 29.943279682567308], 'output': 1.607584798252269e-24, 'delta': -2.5843288835717885e-48}]
node #0: {'weights': [-14.901706013793822, -9.566077118431668, -15.843660282889324, -0.47251888954627835, -16.539356868656355, -14.759586870933012, -6.323594556719743, -17.24367000191697, -4.039609552936097, -54.0171167911726], 'output': 1.0, 'delta': 0.0}
node #1: {'weights': [31.38219642990017, 8.434499927579063, 18.17271544984127, -7.789699522381183, -16.369254379338013, 23.138328981397684, 3.0960515403605795, 20.69813457437844, 0.33000390101636634, 29.943279682567308], 'output': 1.607584798252269e-24, 'delta': -2.5843288835717885e-48}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.500, error: 131.75563249
epoch: 100, lrate: 0.500, error: 39.83895985
epoch: 200, lrate: 0.500, error: 34.32244707
epoch: 300, lrate: 0.500, error: 30.50856365
epoch: 400, lrate: 0.500, error: 28.35712787
epoch: 500, lrate: 0.500, error: 27.00524776
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-3.4797120891915965, -4.146238471549451, -2.417573389549592, -0.6016438667261038, -2.388681286464553, -3.656544380669491, -1.614070346632523, -4.180736839456413, -1.3009004211631556, -12.884993976927303], 'output': 0.999968650755681, 'delta': 9.827443101164871e-10}, {'weights': [3.4541307553463128, 4.140977315188804, 2.404207504055766, 0.5984338021920645, 2.4136766168151254, 3.664481707947191, 1.6305739429629953, 4.176786973819879, 1.3338428626796965, 12.921818794066617], 'output': 3.1707674494114464e-05, 'delta': -1.0053447436700482e-09}]
node #0: {'weights': [-3.4797120891915965, -4.146238471549451, -2.417573389549592, -0.6016438667261038, -2.388681286464553, -3.656544380669491, -1.614070346632523, -4.180736839456413, -1.3009004211631556, -12.884993976927303], 'output': 0.999968650755681, 'delta': 9.827443101164871e-10}
node #1: {'weights': [3.4541307553463128, 4.140977315188804, 2.404207504055766, 0.5984338021920645, 2.4136766168151254, 3.664481707947191, 1.6305739429629953, 4.176786973819879, 1.3338428626796965, 12.921818794066617], 'output': 3.1707674494114464e-05, 'delta': -1.0053447436700482e-09}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.500, error: 142.64826989
epoch: 100, lrate: 0.500, error: 39.83478602
epoch: 200, lrate: 0.500, error: 34.33177498
epoch: 300, lrate: 0.500, error: 30.51417212
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.84065041785285, -3.3596287278708385, -1.480417885118143, -0.6865321566651508, -1.328204853385346, -2.8938665772864334, -1.5606842935125933, -3.3689716570093657, -1.1619132555274145, -10.066798854034175], 'output': 0.9997561989128004, 'delta': 5.942447883415456e-08}, {'weights': [2.788742016278305, 3.3324201991941744, 1.456882287148574, 0.6778419289771632, 1.3745399333211137, 2.9026938068923873, 1.5810441094516339, 3.364700970866901, 1.2177450775380656, 10.113061128483524], 'output': 0.000253955993151726, 'delta': -6.447726790964137e-08}]
node #0: {'weights': [-2.84065041785285, -3.3596287278708385, -1.480417885118143, -0.6865321566651508, -1.328204853385346, -2.8938665772864334, -1.5606842935125933, -3.3689716570093657, -1.1619132555274145, -10.066798854034175], 'output': 0.9997561989128004, 'delta': 5.942447883415456e-08}
node #1: {'weights': [2.788742016278305, 3.3324201991941744, 1.456882287148574, 0.6778419289771632, 1.3745399333211137, 2.9026938068923873, 1.5810441094516339, 3.364700970866901, 1.2177450775380656, 10.113061128483524], 'output': 0.000253955993151726, 'delta': -6.447726790964137e-08}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.100, error: 253.77321358
epoch: 100, lrate: 0.100, error: 52.03715359
epoch: 200, lrate: 0.100, error: 46.78482576
epoch: 300, lrate: 0.100, error: 44.00068820
epoch: 400, lrate: 0.100, error: 42.19674637
epoch: 500, lrate: 0.100, error: 40.89385560
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.6979564447445252, -2.560030953470554, -0.7666299783030647, -0.8343044603403761, -0.10735423552387274, -2.346404069818435, -0.5960981476753402, -1.6831755943805893, -0.9270392083782771, -6.0700456871383315], 'output': 0.9983822313853948, 'delta': 2.61294130635778e-06}, {'weights': [2.6500891600667718, 2.501925986437036, 0.7643863326045743, 0.8310097508365402, 0.13715134158376394, 2.331034022747576, 0.6165546985232877, 1.6791951818619484, 0.9976315438201887, 6.107125561995923], 'output': 0.001706893991433718, 'delta': -2.908514084370846e-06}]
node #0: {'weights': [-2.6979564447445252, -2.560030953470554, -0.7666299783030647, -0.8343044603403761, -0.10735423552387274, -2.346404069818435, -0.5960981476753402, -1.6831755943805893, -0.9270392083782771, -6.0700456871383315], 'output': 0.9983822313853948, 'delta': 2.61294130635778e-06}
node #1: {'weights': [2.6500891600667718, 2.501925986437036, 0.7643863326045743, 0.8310097508365402, 0.13715134158376394, 2.331034022747576, 0.6165546985232877, 1.6791951818619484, 0.9976315438201887, 6.107125561995923], 'output': 0.001706893991433718, 'delta': -2.908514084370846e-06}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.100, error: 222.19578114
epoch: 100, lrate: 0.100, error: 51.87006060
epoch: 200, lrate: 0.100, error: 46.69540401
epoch: 300, lrate: 0.100, error: 43.95192653
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.4685284280665853, -2.103200305281633, -0.874715695129004, -0.7832762828875753, -0.1886117899345363, -2.209718976289573, -0.5703491012871127, -1.4660901423809671, -0.6375682579770489, -5.281762327672034], 'output': 0.9974734651428032, 'delta': 6.367250556634922e-06}, {'weights': [2.3866355968005477, 1.9898699161292224, 0.8710993211204993, 0.7852565051071914, 0.23913554233315001, 2.1780721653779493, 0.620273707224185, 1.457094248708849, 0.7505743405377502, 5.335911547159554], 'output': 0.0027622290253177706, 'delta': -7.608833631687479e-06}]
node #0: {'weights': [-2.4685284280665853, -2.103200305281633, -0.874715695129004, -0.7832762828875753, -0.1886117899345363, -2.209718976289573, -0.5703491012871127, -1.4660901423809671, -0.6375682579770489, -5.281762327672034], 'output': 0.9974734651428032, 'delta': 6.367250556634922e-06}
node #1: {'weights': [2.3866355968005477, 1.9898699161292224, 0.8710993211204993, 0.7852565051071914, 0.23913554233315001, 2.1780721653779493, 0.620273707224185, 1.457094248708849, 0.7505743405377502, 5.335911547159554], 'output': 0.0027622290253177706, 'delta': -7.608833631687479e-06}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
The tuned model looks like: l_rate: 0.1; n_epoch: 500, n_hidden: []
The model has a loss of 1.4492753623188406
epoch: 0, lrate: 0.100, error: 242.43065450
epoch: 100, lrate: 0.100, error: 51.36740318
epoch: 200, lrate: 0.100, error: 46.31422114
epoch: 300, lrate: 0.100, error: 43.67549100
epoch: 400, lrate: 0.100, error: 41.95876304
epoch: 500, lrate: 0.100, error: 40.71005190
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.7110912904888713, -2.58436375690777, -0.756080823947149, -0.8364264614959056, -0.10210459100505456, -2.352677002318761, -0.594152285758698, -1.689871279395223, -0.9219580115752086, -6.082123941695067], 'output': 0.9984123465175403, 'delta': 2.516641671808022e-06}, {'weights': [2.6821877883658445, 2.559856461014058, 0.7467207167966519, 0.8374847317443318, 0.12512297488971785, 2.3474330692917666, 0.6168993331921744, 1.701766570147511, 1.011801739087486, 6.1758518254685635], 'output': 0.0016169199981825903, 'delta': -2.610202955918356e-06}]
node #0: {'weights': [-2.7110912904888713, -2.58436375690777, -0.756080823947149, -0.8364264614959056, -0.10210459100505456, -2.352677002318761, -0.594152285758698, -1.689871279395223, -0.9219580115752086, -6.082123941695067], 'output': 0.9984123465175403, 'delta': 2.516641671808022e-06}
node #1: {'weights': [2.6821877883658445, 2.559856461014058, 0.7467207167966519, 0.8374847317443318, 0.12512297488971785, 2.3474330692917666, 0.6168993331921744, 1.701766570147511, 1.011801739087486, 6.1758518254685635], 'output': 0.0016169199981825903, 'delta': -2.610202955918356e-06}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 10.000, error: 44.87526427
epoch: 100, lrate: 10.000, error: 16.83199233
epoch: 200, lrate: 10.000, error: 17.09196915
epoch: 300, lrate: 10.000, error: 15.72057875
epoch: 400, lrate: 10.000, error: 15.65924318
epoch: 500, lrate: 10.000, error: 15.61438412
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-5.805469855772126, 0.08416777739076917, -14.001335654471163, -3.488038625713778, 6.3995858703437465, -10.062060309441874, -10.969374750420245, -9.094451805940915, -2.996756394978403, -23.733609726176333], 'output': 0.9999999999989913, 'delta': 1.0175738190527094e-24}, {'weights': [6.055296292208677, 12.16134557234436, 14.71431629501151, 3.571035417902766, -2.01699020336713, 17.17973875674918, 18.062997121296448, 2.342556141117092, 3.925765780689269, 39.08750507742449], 'output': 5.97252890076196e-17, 'delta': -3.5671101470436853e-33}]
node #0: {'weights': [-5.805469855772126, 0.08416777739076917, -14.001335654471163, -3.488038625713778, 6.3995858703437465, -10.062060309441874, -10.969374750420245, -9.094451805940915, -2.996756394978403, -23.733609726176333], 'output': 0.9999999999989913, 'delta': 1.0175738190527094e-24}
node #1: {'weights': [6.055296292208677, 12.16134557234436, 14.71431629501151, 3.571035417902766, -2.01699020336713, 17.17973875674918, 18.062997121296448, 2.342556141117092, 3.925765780689269, 39.08750507742449], 'output': 5.97252890076196e-17, 'delta': -3.5671101470436853e-33}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 10.000, error: 49.87319463
epoch: 100, lrate: 10.000, error: 16.78897335
epoch: 200, lrate: 10.000, error: 16.43929760
epoch: 300, lrate: 10.000, error: 15.72760356
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-6.337457847188961, -11.819679256927795, -14.642098551417064, -3.752552234244172, 2.2978784911091172, -16.01861796238656, -16.532757941114074, -2.5461144170779093, -3.766098802787243, -37.30034532179163], 'output': 0.9999999999999998, 'delta': 4.9303806576313227e-32}, {'weights': [4.781236055798373, 0.25437657656343865, 13.181388719969322, 2.983621568467927, -5.968357917745751, 9.542451382434745, 10.504206023010514, 7.846155265958834, 3.430130966884391, 22.9034397917313], 'output': 1.4195861902275502e-11, 'delta': -2.0152249514561626e-22}]
node #0: {'weights': [-6.337457847188961, -11.819679256927795, -14.642098551417064, -3.752552234244172, 2.2978784911091172, -16.01861796238656, -16.532757941114074, -2.5461144170779093, -3.766098802787243, -37.30034532179163], 'output': 0.9999999999999998, 'delta': 4.9303806576313227e-32}
node #1: {'weights': [4.781236055798373, 0.25437657656343865, 13.181388719969322, 2.983621568467927, -5.968357917745751, 9.542451382434745, 10.504206023010514, 7.846155265958834, 3.430130966884391, 22.9034397917313], 'output': 1.4195861902275502e-11, 'delta': -2.0152249514561626e-22}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.500, error: 117.41155532
epoch: 100, lrate: 0.500, error: 29.89610960
epoch: 200, lrate: 0.500, error: 25.82353421
epoch: 300, lrate: 0.500, error: 24.31621643
epoch: 400, lrate: 0.500, error: 23.51212157
epoch: 500, lrate: 0.500, error: 22.52743029
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-4.473045640778815, -4.9460042685043, -3.8575303049610734, -1.2313973727052057, 0.42986648503381997, -5.0713050548041725, 2.1843105476150813, -2.5764417256780323, -4.801808594749804, -12.38053147366292], 'output': 0.9999942040788465, 'delta': 3.359250731649544e-11}, {'weights': [4.129364878886705, 4.429704188389414, 3.74501181575981, 1.1508708736072224, -0.39564174774448946, 4.685195427242256, -1.839508454554227, 2.3907223518540963, 4.371433042173201, 11.485760758430288], 'output': 1.2758347901695873e-05, 'delta': -1.6277336443499903e-10}]
node #0: {'weights': [-4.473045640778815, -4.9460042685043, -3.8575303049610734, -1.2313973727052057, 0.42986648503381997, -5.0713050548041725, 2.1843105476150813, -2.5764417256780323, -4.801808594749804, -12.38053147366292], 'output': 0.9999942040788465, 'delta': 3.359250731649544e-11}
node #1: {'weights': [4.129364878886705, 4.429704188389414, 3.74501181575981, 1.1508708736072224, -0.39564174774448946, 4.685195427242256, -1.839508454554227, 2.3907223518540963, 4.371433042173201, 11.485760758430288], 'output': 1.2758347901695873e-05, 'delta': -1.6277336443499903e-10}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.500, error: 122.18904062
epoch: 100, lrate: 0.500, error: 29.74043109
epoch: 200, lrate: 0.500, error: 25.82100029
epoch: 300, lrate: 0.500, error: 24.54186170
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-3.36011419008814, -3.37320619519711, -3.303779348672412, -0.9798744604412883, 0.35854683314675, -3.7419425098589056, 0.9121511717605634, -1.961060749935699, -3.269788348264492, -9.359168924312229], 'output': 0.9999204921999973, 'delta': 6.320987653478599e-09}, {'weights': [2.9872157894412807, 2.9897138062191355, 3.541925471659231, 1.0899639528511456, -0.5281920125908182, 3.32802739118898, 0.06680024751676472, 1.7421258679443108, 2.872953688331055, 9.035977225133143], 'output': 0.00010390465704996758, 'delta': -1.0795055983524119e-08}]
node #0: {'weights': [-3.36011419008814, -3.37320619519711, -3.303779348672412, -0.9798744604412883, 0.35854683314675, -3.7419425098589056, 0.9121511717605634, -1.961060749935699, -3.269788348264492, -9.359168924312229], 'output': 0.9999204921999973, 'delta': 6.320987653478599e-09}
node #1: {'weights': [2.9872157894412807, 2.9897138062191355, 3.541925471659231, 1.0899639528511456, -0.5281920125908182, 3.32802739118898, 0.06680024751676472, 1.7421258679443108, 2.872953688331055, 9.035977225133143], 'output': 0.00010390465704996758, 'delta': -1.0795055983524119e-08}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.100, error: 196.56758988
epoch: 100, lrate: 0.100, error: 41.93275138
epoch: 200, lrate: 0.100, error: 37.09849549
epoch: 300, lrate: 0.100, error: 34.10772894
epoch: 400, lrate: 0.100, error: 31.86626176
epoch: 500, lrate: 0.100, error: 30.20246398
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.037776448658101, -1.7584055570171082, -2.736414221320816, -0.9401302752615217, 0.3708125743779639, -2.2332268057087967, -0.9430276502698475, -1.2798287086283446, -1.3744707414215858, -6.070234589260367], 'output': 0.9990371360920627, 'delta': 9.262142274304974e-07}, {'weights': [1.935796188286699, 1.6264251342981224, 2.747328481672436, 0.956095152282587, -0.3774106142098628, 2.137258376980151, 1.1630177262718127, 1.2158047969738524, 1.3465131008081983, 6.010425908353033], 'output': 0.001085888419228447, 'delta': -1.1778732297116408e-06}]
node #0: {'weights': [-2.037776448658101, -1.7584055570171082, -2.736414221320816, -0.9401302752615217, 0.3708125743779639, -2.2332268057087967, -0.9430276502698475, -1.2798287086283446, -1.3744707414215858, -6.070234589260367], 'output': 0.9990371360920627, 'delta': 9.262142274304974e-07}
node #1: {'weights': [1.935796188286699, 1.6264251342981224, 2.747328481672436, 0.956095152282587, -0.3774106142098628, 2.137258376980151, 1.1630177262718127, 1.2158047969738524, 1.3465131008081983, 6.010425908353033], 'output': 0.001085888419228447, 'delta': -1.1778732297116408e-06}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.100, error: 232.59929750
epoch: 100, lrate: 0.100, error: 41.98687692
epoch: 200, lrate: 0.100, error: 37.33127286
epoch: 300, lrate: 0.100, error: 34.38836914
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-1.9471433192631422, -1.449678060032145, -2.3051639617432698, -0.9089200570707457, 0.2920412534214983, -1.9078720206490045, -1.2347105753607017, -1.1328380944262533, -0.9937379415055869, -5.246639295247985], 'output': 0.9983515646727683, 'delta': 2.71285967041552e-06}, {'weights': [1.8255695088505324, 1.0817737198299096, 2.2217794229876575, 0.9876644545040395, -0.24874666643267163, 1.680935265702273, 1.667406182286431, 0.9958593345662115, 1.018968658699948, 5.089929637210406], 'output': 0.002032298849382287, 'delta': -4.121844734019285e-06}]
node #0: {'weights': [-1.9471433192631422, -1.449678060032145, -2.3051639617432698, -0.9089200570707457, 0.2920412534214983, -1.9078720206490045, -1.2347105753607017, -1.1328380944262533, -0.9937379415055869, -5.246639295247985], 'output': 0.9983515646727683, 'delta': 2.71285967041552e-06}
node #1: {'weights': [1.8255695088505324, 1.0817737198299096, 2.2217794229876575, 0.9876644545040395, -0.24874666643267163, 1.680935265702273, 1.667406182286431, 0.9958593345662115, 1.018968658699948, 5.089929637210406], 'output': 0.002032298849382287, 'delta': -4.121844734019285e-06}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
The tuned model looks like: l_rate: 0.5; n_epoch: 500, n_hidden: []
The model has a loss of 1.4492753623188406
epoch: 0, lrate: 0.500, error: 188.67914628
epoch: 100, lrate: 0.500, error: 29.90775268
epoch: 200, lrate: 0.500, error: 25.73841847
epoch: 300, lrate: 0.500, error: 24.60399260
epoch: 400, lrate: 0.500, error: 23.61367547
epoch: 500, lrate: 0.500, error: 22.54494145
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-4.347824982171462, -4.76380120607855, -3.790884699992703, -1.1952853386494386, 0.4136492901254493, -4.926502176268449, 2.0633754977339867, -2.5081386436135067, -4.633793898178371, -12.031327900614972], 'output': 0.999992109161946, 'delta': 6.22648338683828e-11}, {'weights': [4.252304263833111, 4.620596877509337, 3.757574716637121, 1.1720207542889847, -0.4032499834463305, 4.819560115626191, -1.968916117414201, 2.4563822812070617, 4.514059996405061, 11.781290613063145], 'output': 9.843854142360544e-06, 'delta': -9.690051049218735e-11}]
node #0: {'weights': [-4.347824982171462, -4.76380120607855, -3.790884699992703, -1.1952853386494386, 0.4136492901254493, -4.926502176268449, 2.0633754977339867, -2.5081386436135067, -4.633793898178371, -12.031327900614972], 'output': 0.999992109161946, 'delta': 6.22648338683828e-11}
node #1: {'weights': [4.252304263833111, 4.620596877509337, 3.757574716637121, 1.1720207542889847, -0.4032499834463305, 4.819560115626191, -1.968916117414201, 2.4563822812070617, 4.514059996405061, 11.781290613063145], 'output': 9.843854142360544e-06, 'delta': -9.690051049218735e-11}
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 10.000, error: 54.48966147
epoch: 100, lrate: 10.000, error: 29.47827336
epoch: 200, lrate: 10.000, error: 28.51900774
epoch: 300, lrate: 10.000, error: 28.38076541
epoch: 400, lrate: 10.000, error: 27.99549496
epoch: 500, lrate: 10.000, error: 25.48712759
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-30.892702777499537, -2.988605547103761, -12.558448872195319, -29.623339166725234, 3.075806424595597, -25.362423887251897, -7.910811818851617, -13.367580455989762, -3.4831345992395186, -53.4636961714208], 'output': 1.022931690645836e-40, 'delta': -1.0463892437275482e-80}, {'weights': [30.49846410147548, 3.52705470587457, 11.168550591071028, 29.42146930105049, -2.8501158622019793, 25.352288274840006, 8.048932253266974, 13.47019950484296, 3.888862700477046, 53.52494938109471], 'output': 1.0, 'delta': 0.0}]
node #0: {'weights': [-30.892702777499537, -2.988605547103761, -12.558448872195319, -29.623339166725234, 3.075806424595597, -25.362423887251897, -7.910811818851617, -13.367580455989762, -3.4831345992395186, -53.4636961714208], 'output': 1.022931690645836e-40, 'delta': -1.0463892437275482e-80}
node #1: {'weights': [30.49846410147548, 3.52705470587457, 11.168550591071028, 29.42146930105049, -2.8501158622019793, 25.352288274840006, 8.048932253266974, 13.47019950484296, 3.888862700477046, 53.52494938109471], 'output': 1.0, 'delta': 0.0}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 10; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 10.000, error: 57.28566073
epoch: 100, lrate: 10.000, error: 27.12647371
epoch: 200, lrate: 10.000, error: 28.10120004
epoch: 300, lrate: 10.000, error: 26.81260026
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-21.607550027996265, -3.9502831737022417, -7.4883672248049455, -20.644370531189203, -0.8092327071003509, -21.58614041993592, -10.023994443702403, -14.298345830704772, -0.4358188605157981, -43.98787485868748], 'output': 3.975750321888149e-35, 'delta': -1.5806590621993719e-69}, {'weights': [23.98476914715112, 3.6159096857121344, 7.550435901338504, 22.56384338596164, 0.7375969385612332, 23.06747242518733, 9.339672440352576, 14.990874278867627, 0.6919784834459235, 46.39225029514123], 'output': 1.0, 'delta': 0.0}]
node #0: {'weights': [-21.607550027996265, -3.9502831737022417, -7.4883672248049455, -20.644370531189203, -0.8092327071003509, -21.58614041993592, -10.023994443702403, -14.298345830704772, -0.4358188605157981, -43.98787485868748], 'output': 3.975750321888149e-35, 'delta': -1.5806590621993719e-69}
node #1: {'weights': [23.98476914715112, 3.6159096857121344, 7.550435901338504, 22.56384338596164, 0.7375969385612332, 23.06747242518733, 9.339672440352576, 14.990874278867627, 0.6919784834459235, 46.39225029514123], 'output': 1.0, 'delta': 0.0}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.500, error: 116.94358563
epoch: 100, lrate: 0.500, error: 49.33052193
epoch: 200, lrate: 0.500, error: 44.01850994
epoch: 300, lrate: 0.500, error: 41.99795256
epoch: 400, lrate: 0.500, error: 40.85245937
epoch: 500, lrate: 0.500, error: 40.07874554
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-3.3833863759868428, -1.1119159557706362, 1.3925038973334036, -2.9677116280274394, -1.1481828194431085, -3.0692783312564496, -1.9615230207609804, -1.0384359506413188, -3.675446457531409, -9.004202843452362], 'output': 0.0005286608174694793, 'delta': -2.7933450860749663e-07}, {'weights': [3.4065842048913293, 1.129794963171689, -1.419650450948425, 2.9834211373375252, 1.1517571388667356, 3.090153937076704, 1.9676425997800808, 1.047568245797465, 3.6964677476699506, 9.05350821000467], 'output': 0.999488809415785, 'delta': 2.6118223120676604e-07}]
node #0: {'weights': [-3.3833863759868428, -1.1119159557706362, 1.3925038973334036, -2.9677116280274394, -1.1481828194431085, -3.0692783312564496, -1.9615230207609804, -1.0384359506413188, -3.675446457531409, -9.004202843452362], 'output': 0.0005286608174694793, 'delta': -2.7933450860749663e-07}
node #1: {'weights': [3.4065842048913293, 1.129794963171689, -1.419650450948425, 2.9834211373375252, 1.1517571388667356, 3.090153937076704, 1.9676425997800808, 1.047568245797465, 3.6964677476699506, 9.05350821000467], 'output': 0.999488809415785, 'delta': 2.6118223120676604e-07}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.5; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.500, error: 132.86839153
epoch: 100, lrate: 0.500, error: 49.51583633
epoch: 200, lrate: 0.500, error: 44.07731486
epoch: 300, lrate: 0.500, error: 42.02787897
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-2.964210458132631, -0.7798064686741161, 0.887757109409559, -2.683791955540513, -1.0765132947685434, -2.6916497167717544, -1.8638142681693328, -0.8874921850895149, -3.297539029964919, -8.126495674950908], 'output': 0.0009362220117245597, 'delta': -8.75691045732415e-07}, {'weights': [2.996866178555487, 0.8056423264543956, -0.9273909957827932, 2.7061548699765567, 1.0825862707887624, 2.721144273062981, 1.8705446999880508, 0.8981992662444531, 3.327110062757335, 8.194280202482028], 'output': 0.9991024385026993, 'delta': 8.048935509577215e-07}]
node #0: {'weights': [-2.964210458132631, -0.7798064686741161, 0.887757109409559, -2.683791955540513, -1.0765132947685434, -2.6916497167717544, -1.8638142681693328, -0.8874921850895149, -3.297539029964919, -8.126495674950908], 'output': 0.0009362220117245597, 'delta': -8.75691045732415e-07}
node #1: {'weights': [2.996866178555487, 0.8056423264543956, -0.9273909957827932, 2.7061548699765567, 1.0825862707887624, 2.721144273062981, 1.8705446999880508, 0.8981992662444531, 3.327110062757335, 8.194280202482028], 'output': 0.9991024385026993, 'delta': 8.048935509577215e-07}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 500, n_hidden: []
epoch: 0, lrate: 0.100, error: 199.31677860
epoch: 100, lrate: 0.100, error: 58.55472447
epoch: 200, lrate: 0.100, error: 58.03151170
epoch: 300, lrate: 0.100, error: 56.32583587
epoch: 400, lrate: 0.100, error: 54.30141710
epoch: 500, lrate: 0.100, error: 52.61180950
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-1.6937296968828777, -0.4349249580401344, -0.2698419242985502, -1.5601787807197085, -0.5859337161775151, -1.6071588380638322, -1.572118686023218, -0.770495700326337, -2.259505901510301, -5.259645355317668], 'output': 0.004345202738184212, 'delta': -1.8798745989265046e-05}, {'weights': [1.7212452606201356, 0.4349680671388899, 0.24848845556351823, 1.5948044377630604, 0.5981030470004125, 1.6297615630891484, 1.579568961202696, 0.772461783336944, 2.314262591524739, 5.347674179675992], 'output': 0.9957773284577552, 'delta': 1.7755660687629774e-05}]
node #0: {'weights': [-1.6937296968828777, -0.4349249580401344, -0.2698419242985502, -1.5601787807197085, -0.5859337161775151, -1.6071588380638322, -1.572118686023218, -0.770495700326337, -2.259505901510301, -5.259645355317668], 'output': 0.004345202738184212, 'delta': -1.8798745989265046e-05}
node #1: {'weights': [1.7212452606201356, 0.4349680671388899, 0.24848845556351823, 1.5948044377630604, 0.5981030470004125, 1.6297615630891484, 1.579568961202696, 0.772461783336944, 2.314262591524739, 5.347674179675992], 'output': 0.9957773284577552, 'delta': 1.7755660687629774e-05}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
l_rate: 0.1; n_epoch: 300, n_hidden: []
epoch: 0, lrate: 0.100, error: 203.12546024
epoch: 100, lrate: 0.100, error: 58.81388369
epoch: 200, lrate: 0.100, error: 57.97751239
epoch: 300, lrate: 0.100, error: 56.13782454
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-1.5465612425497437, -0.4058950241953069, -0.45524703622796653, -1.3261583960066101, -0.4916569288148284, -1.4565517724469685, -1.4996023775246734, -0.7504172271478698, -1.6641095223219387, -4.448199885710401], 'output': 0.005251641877935781, 'delta': -2.7434903483844344e-05}, {'weights': [1.5540285891701109, 0.3813438667921963, 0.4587907321295002, 1.3537780028669772, 0.5004703471068835, 1.466188586784708, 1.5136732701550883, 0.7538566498385316, 1.7465830333457915, 4.547775525166552], 'output': 0.9948348284314199, 'delta': 2.6541195734366214e-05}]
node #0: {'weights': [-1.5465612425497437, -0.4058950241953069, -0.45524703622796653, -1.3261583960066101, -0.4916569288148284, -1.4565517724469685, -1.4996023775246734, -0.7504172271478698, -1.6641095223219387, -4.448199885710401], 'output': 0.005251641877935781, 'delta': -2.7434903483844344e-05}
node #1: {'weights': [1.5540285891701109, 0.3813438667921963, 0.4587907321295002, 1.3537780028669772, 0.5004703471068835, 1.466188586784708, 1.5136732701550883, 0.7538566498385316, 1.7465830333457915, 4.547775525166552], 'output': 0.9948348284314199, 'delta': 2.6541195734366214e-05}
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
The tuned model looks like: l_rate: 0.1; n_epoch: 500, n_hidden: []
The model has a loss of 1.4492753623188406
epoch: 0, lrate: 0.100, error: 223.12175978
epoch: 100, lrate: 0.100, error: 58.23147616
epoch: 200, lrate: 0.100, error: 57.90451607
epoch: 300, lrate: 0.100, error: 56.26945768
epoch: 400, lrate: 0.100, error: 54.25267000
epoch: 500, lrate: 0.100, error: 52.57225856
the trained network:
there are a total of 0 hidden layers
there are 9 inputs
there are 2 nodes in the output layer
output layer: [{'weights': [-1.6875327362699675, -0.4357234677157023, -0.27404074213844304, -1.5518919938692743, -0.5830512143095775, -1.6020457675833584, -1.5699060058772911, -0.7699539360726104, -2.245414727863091, -5.237746859565782], 'output': 0.004376098935638477, 'delta': -1.906643854132449e-05}, {'weights': [1.7352052772921702, 0.44539037565674344, 0.22851132844231353, 1.6087095619684644, 0.6039465669110645, 1.6418830521441001, 1.5801931736904817, 0.7729521124717103, 2.334787271881109, 5.383688195893114], 'output': 0.9958225440625788, 'delta': 1.737823674858744e-05}]
node #0: {'weights': [-1.6875327362699675, -0.4357234677157023, -0.27404074213844304, -1.5518919938692743, -0.5830512143095775, -1.6020457675833584, -1.5699060058772911, -0.7699539360726104, -2.245414727863091, -5.237746859565782], 'output': 0.004376098935638477, 'delta': -1.906643854132449e-05}
node #1: {'weights': [1.7352052772921702, 0.44539037565674344, 0.22851132844231353, 1.6087095619684644, 0.6039465669110645, 1.6418830521441001, 1.5801931736904817, 0.7729521124717103, 2.334787271881109, 5.383688195893114], 'output': 0.9958225440625788, 'delta': 1.737823674858744e-05}
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
predicted: 4.0; actual: 4.0
predicted: 2.0; actual: 2.0
predicted: 2.0; actual: 2.0
The set was breast-cancer-wisconsin
Error by fold: [5.691056910569105, 1.6260162601626018, 2.4390243902439024, 6.504065040650407, 1.639344262295082]
% Incorrect: 3.580%
